{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParlAI Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsb-Cvf6lnVX"
      },
      "source": [
        "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
        "\n",
        "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
        "\n",
        "\n",
        "# Welcome to the ParlAI interactive tutorial\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "- Chat with a neural network model!\n",
        "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
        "- See where to find information about many options.\n",
        "- Show how to fine-tune a pretrained model on a specific task\n",
        "- Add our own datasets to ParlAI\n",
        "- And add our own models to ParlAI\n",
        "\n",
        "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
        "\n",
        "**Note:** *Make sure you're running this session with a GPU attached.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bFnOWslsj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbc5ec3-0284-42fa-f1dd-240ac5a67bde"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb  7 12:56:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxd1KIRl9Xm"
      },
      "source": [
        "## Installing parlai\n",
        "\n",
        "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93Mn_I7MOEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131b8ca1-180b-4506-c611-f239fbe0c0ff"
      },
      "source": [
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 26.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 27.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 35.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.2MB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 56.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 60.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 22.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 64.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 51.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25h  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210204 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.13.3 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVz5dCUmFkN"
      },
      "source": [
        "# Chatting with a model\n",
        "\n",
        "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGRtMKmIWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d6c864-2b1a-4278-ec1e-55165e46dc3f"
      },
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.3) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12:57:21 | building data: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "12:57:21 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:15<00:00, 71.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12:57:55 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
            "12:57:55 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "12:57:55 | Using CUDA\n",
            "12:57:55 | \u001b[31mYou set --fp16 true with --fp16-impl apex, but fp16 with apex is unavailable. To use apex fp16, please install APEX from https://github.com/NVIDIA/apex.\u001b[0m\n",
            "12:57:55 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "12:57:55 | num words = 54944\n",
            "12:57:56 | TransformerGenerator: full interactive mode on.\n",
            "12:57:57 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "12:58:08 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "12:58:08 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "12:58:09 | Opt:\n",
            "12:58:09 |     activation: gelu\n",
            "12:58:09 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "12:58:09 |     adam_eps: 1e-06\n",
            "12:58:09 |     add_p1_after_newln: False\n",
            "12:58:09 |     aggregate_micro: False\n",
            "12:58:09 |     allow_missing_init_opts: False\n",
            "12:58:09 |     attention_dropout: 0.0\n",
            "12:58:09 |     batch_length_range: 5\n",
            "12:58:09 |     batch_sort_cache_type: pop\n",
            "12:58:09 |     batch_sort_field: text\n",
            "12:58:09 |     batchsize: 48\n",
            "12:58:09 |     beam_block_full_context: False\n",
            "12:58:09 |     beam_block_list_filename: None\n",
            "12:58:09 |     beam_block_ngram: 3\n",
            "12:58:09 |     beam_context_block_ngram: 3\n",
            "12:58:09 |     beam_delay: 30\n",
            "12:58:09 |     beam_length_penalty: 0.65\n",
            "12:58:09 |     beam_min_length: 10\n",
            "12:58:09 |     beam_min_n_best: 3\n",
            "12:58:09 |     beam_size: 8\n",
            "12:58:09 |     betas: '[0.9, 0.98]'\n",
            "12:58:09 |     bpe_add_prefix_space: None\n",
            "12:58:09 |     bpe_debug: False\n",
            "12:58:09 |     bpe_merge: None\n",
            "12:58:09 |     bpe_vocab: None\n",
            "12:58:09 |     compute_tokenized_bleu: False\n",
            "12:58:09 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "12:58:09 |     datatype: train:stream\n",
            "12:58:09 |     delimiter: '\\n'\n",
            "12:58:09 |     dict_build_first: True\n",
            "12:58:09 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "12:58:09 |     dict_endtoken: __end__\n",
            "12:58:09 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "12:58:09 |     dict_include_test: False\n",
            "12:58:09 |     dict_include_valid: False\n",
            "12:58:09 |     dict_initpath: None\n",
            "12:58:09 |     dict_language: english\n",
            "12:58:09 |     dict_loaded: True\n",
            "12:58:09 |     dict_lower: True\n",
            "12:58:09 |     dict_max_ngram_size: -1\n",
            "12:58:09 |     dict_maxexs: -1\n",
            "12:58:09 |     dict_maxtokens: -1\n",
            "12:58:09 |     dict_minfreq: 0\n",
            "12:58:09 |     dict_nulltoken: __null__\n",
            "12:58:09 |     dict_starttoken: __start__\n",
            "12:58:09 |     dict_textfields: text,labels\n",
            "12:58:09 |     dict_tokenizer: bpe\n",
            "12:58:09 |     dict_unktoken: __unk__\n",
            "12:58:09 |     display_examples: False\n",
            "12:58:09 |     display_ignore_fields: label_candidates,text_candidates\n",
            "12:58:09 |     display_prettify: False\n",
            "12:58:09 |     distributed_world_size: 64\n",
            "12:58:09 |     download_path: None\n",
            "12:58:09 |     dropout: 0.1\n",
            "12:58:09 |     dynamic_batching: None\n",
            "12:58:09 |     embedding_projection: random\n",
            "12:58:09 |     embedding_size: 512\n",
            "12:58:09 |     embedding_type: random\n",
            "12:58:09 |     embeddings_scale: True\n",
            "12:58:09 |     eval_batchsize: None\n",
            "12:58:09 |     evaltask: None\n",
            "12:58:09 |     ffn_size: 2048\n",
            "12:58:09 |     force_fp16_tokens: True\n",
            "12:58:09 |     fp16: True\n",
            "12:58:09 |     fp16_impl: apex\n",
            "12:58:09 |     gpu: 0\n",
            "12:58:09 |     gradient_clip: 10.0\n",
            "12:58:09 |     hf_skip_special_tokens: True\n",
            "12:58:09 |     hide_labels: False\n",
            "12:58:09 |     history_add_global_end_token: None\n",
            "12:58:09 |     history_reversed: False\n",
            "12:58:09 |     history_size: -1\n",
            "12:58:09 |     image_cropsize: 224\n",
            "12:58:09 |     image_mode: raw\n",
            "12:58:09 |     image_size: 256\n",
            "12:58:09 |     inference: beam\n",
            "12:58:09 |     init_model: None\n",
            "12:58:09 |     init_opt: None\n",
            "12:58:09 |     interactive_mode: True\n",
            "12:58:09 |     interactive_task: True\n",
            "12:58:09 |     invsqrt_lr_decay_gamma: -1\n",
            "12:58:09 |     label_truncate: 128\n",
            "12:58:09 |     learn_positional_embeddings: True\n",
            "12:58:09 |     learningrate: 0.0005\n",
            "12:58:09 |     load_from_checkpoint: True\n",
            "12:58:09 |     local_human_candidates_file: None\n",
            "12:58:09 |     log_every_n_secs: 30.0\n",
            "12:58:09 |     log_keep_fields: all\n",
            "12:58:09 |     loglevel: info\n",
            "12:58:09 |     lr_scheduler: invsqrt\n",
            "12:58:09 |     lr_scheduler_decay: 0.5\n",
            "12:58:09 |     lr_scheduler_patience: 3\n",
            "12:58:09 |     max_lr_steps: -1\n",
            "12:58:09 |     max_train_time: -1\n",
            "12:58:09 |     metrics: default\n",
            "12:58:09 |     model: transformer/generator\n",
            "12:58:09 |     model_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "12:58:09 |     model_parallel: False\n",
            "12:58:09 |     momentum: 0\n",
            "12:58:09 |     multitask_weights: [1]\n",
            "12:58:09 |     n_decoder_layers: -1\n",
            "12:58:09 |     n_encoder_layers: -1\n",
            "12:58:09 |     n_heads: 16\n",
            "12:58:09 |     n_layers: 8\n",
            "12:58:09 |     n_positions: 512\n",
            "12:58:09 |     n_segments: 0\n",
            "12:58:09 |     nesterov: True\n",
            "12:58:09 |     no_cuda: False\n",
            "12:58:09 |     num_epochs: 5.0\n",
            "12:58:09 |     numthreads: 1\n",
            "12:58:09 |     numworkers: 4\n",
            "12:58:09 |     nus: [0.7]\n",
            "12:58:09 |     optimizer: fused_adam\n",
            "12:58:09 |     outfile: \n",
            "12:58:09 |     output_scaling: 1.0\n",
            "12:58:09 |     override: \"{'model_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model'}\"\n",
            "12:58:09 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "12:58:09 |     person_tokens: False\n",
            "12:58:09 |     port: 61337\n",
            "12:58:09 |     pytorch_context_length: -1\n",
            "12:58:09 |     pytorch_datapath: None\n",
            "12:58:09 |     pytorch_include_labels: True\n",
            "12:58:09 |     pytorch_preprocess: False\n",
            "12:58:09 |     pytorch_teacher_batch_sort: False\n",
            "12:58:09 |     pytorch_teacher_dataset: None\n",
            "12:58:09 |     pytorch_teacher_task: None\n",
            "12:58:09 |     rank_candidates: False\n",
            "12:58:09 |     relu_dropout: 0.0\n",
            "12:58:09 |     save_after_valid: True\n",
            "12:58:09 |     save_every_n_secs: -1\n",
            "12:58:09 |     save_format: conversations\n",
            "12:58:09 |     share_word_embeddings: True\n",
            "12:58:09 |     short_final_eval: True\n",
            "12:58:09 |     show_advanced_args: False\n",
            "12:58:09 |     shuffle: False\n",
            "12:58:09 |     single_turn: False\n",
            "12:58:09 |     skip_generation: False\n",
            "12:58:09 |     special_tok_lst: None\n",
            "12:58:09 |     split_lines: False\n",
            "12:58:09 |     starttime: Feb07_12-57\n",
            "12:58:09 |     task: internal:new_reddit:presorted\n",
            "12:58:09 |     temperature: 1.0\n",
            "12:58:09 |     tensorboard_log: False\n",
            "12:58:09 |     text_truncate: 512\n",
            "12:58:09 |     topk: 10\n",
            "12:58:09 |     topp: 0.9\n",
            "12:58:09 |     truncate: -1\n",
            "12:58:09 |     update_freq: 1\n",
            "12:58:09 |     use_reply: label\n",
            "12:58:09 |     validation_cutoff: 1.0\n",
            "12:58:09 |     validation_every_n_epochs: -1\n",
            "12:58:09 |     validation_every_n_secs: 1800.0\n",
            "12:58:09 |     validation_max_exs: 9920\n",
            "12:58:09 |     validation_metric: ppl\n",
            "12:58:09 |     validation_metric_mode: min\n",
            "12:58:09 |     validation_patience: 0\n",
            "12:58:09 |     validation_share_agent: False\n",
            "12:58:09 |     variant: xlm\n",
            "12:58:09 |     verbose: False\n",
            "12:58:09 |     warmup_rate: 0.0001\n",
            "12:58:09 |     warmup_updates: 20000\n",
            "12:58:09 |     weight_decay: 0.01\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "12:58:09 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m Hello\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1m+ / u / dogetipbot 10 doge\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you today?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi ' m doing well , how about you ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m fine\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat ' s good to hear , i ' ve been having a lot of trouble sleeping lately .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m why?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi don ' t know , it ' s been a while .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m interesting.\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1myeah , i guess i ' ll just have to get used to it .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m don't you want to fix?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi guess , but i ' d rather not .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m why?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mbecause i can ' t do anything about it .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m maybe you could get some pills about that\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1myeah , that ' d be a good idea .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m exit\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi guess you ' ll have to do that .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m done\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat ' s the point of being here then ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m DONE\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1myou ' re a good person , i like you .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [DONE]\n",
            "CHAT DONE \n",
            "\n",
            "... preparing new chat... \n",
            "\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfUEgovmWay"
      },
      "source": [
        "The same on the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hGrZGGmaWF"
      },
      "source": [
        "# Taking a look at some data\n",
        "\n",
        "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqckSXqlmWuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be65e899-e56d-4c10-fc82-196184b82e7e"
      },
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:02:12 | Opt:\n",
            "13:02:12 |     allow_missing_init_opts: False\n",
            "13:02:12 |     batchsize: 1\n",
            "13:02:12 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:02:12 |     datatype: train:ordered\n",
            "13:02:12 |     dict_class: None\n",
            "13:02:12 |     display_ignore_fields: agent_reply\n",
            "13:02:12 |     display_verbose: False\n",
            "13:02:12 |     download_path: None\n",
            "13:02:12 |     dynamic_batching: None\n",
            "13:02:12 |     hide_labels: False\n",
            "13:02:12 |     image_cropsize: 224\n",
            "13:02:12 |     image_mode: raw\n",
            "13:02:12 |     image_size: 256\n",
            "13:02:12 |     init_model: None\n",
            "13:02:12 |     init_opt: None\n",
            "13:02:12 |     loglevel: info\n",
            "13:02:12 |     max_display_len: 1000\n",
            "13:02:12 |     model: None\n",
            "13:02:12 |     model_file: None\n",
            "13:02:12 |     multitask_weights: [1]\n",
            "13:02:12 |     num_examples: 5\n",
            "13:02:12 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "13:02:12 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:02:12 |     remove_political_convos: False\n",
            "13:02:12 |     starttime: Feb07_13-02\n",
            "13:02:12 |     task: empathetic_dialogues\n",
            "13:02:12 |     train_experiencer_only: False\n",
            "13:02:13 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.6/dist-packages/data/empatheticdialogues]\n",
            "13:02:13 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.6/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:00<00:00, 28.6MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "13:02:15 | loaded 39057 episodes with a total of 64636 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9C6oHq87zGx"
      },
      "source": [
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can also ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNSBetWmfGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df664da4-1052-4c25-8ded-5f768a855d9b"
      },
      "source": [
        "# we can instead ask to see fewer examples, and get them from the valid set.\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:03:12 | Opt:\n",
            "13:03:12 |     allow_missing_init_opts: False\n",
            "13:03:12 |     batchsize: 1\n",
            "13:03:12 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:03:12 |     datatype: valid\n",
            "13:03:12 |     dict_class: None\n",
            "13:03:12 |     display_ignore_fields: agent_reply\n",
            "13:03:12 |     display_verbose: False\n",
            "13:03:12 |     download_path: None\n",
            "13:03:12 |     dynamic_batching: None\n",
            "13:03:12 |     hide_labels: False\n",
            "13:03:12 |     image_cropsize: 224\n",
            "13:03:12 |     image_mode: raw\n",
            "13:03:12 |     image_size: 256\n",
            "13:03:12 |     init_model: None\n",
            "13:03:12 |     init_opt: None\n",
            "13:03:12 |     loglevel: info\n",
            "13:03:12 |     max_display_len: 1000\n",
            "13:03:12 |     model: None\n",
            "13:03:12 |     model_file: None\n",
            "13:03:12 |     multitask_weights: [1]\n",
            "13:03:12 |     num_examples: 3\n",
            "13:03:12 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
            "13:03:12 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:03:12 |     remove_political_convos: False\n",
            "13:03:12 |     starttime: Feb07_13-03\n",
            "13:03:12 |     task: empathetic_dialogues\n",
            "13:03:12 |     train_experiencer_only: False\n",
            "13:03:12 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "13:03:12 | loaded 2769 episodes with a total of 5738 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrgRrEmdS-"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M8Zr86n2_G"
      },
      "source": [
        "# Training a model\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhVQycSn2q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85715c6e-a755-4bd0-8b8c-c8b4bfe8eff4"
      },
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file='from_scratch_model/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    max_train_time=2 * 60,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq',\n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:03:31 | building dictionary first...\n",
            "13:03:31 | Opt:\n",
            "13:03:31 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:03:31 |     adam_eps: 1e-08\n",
            "13:03:31 |     add_p1_after_newln: False\n",
            "13:03:31 |     aggregate_micro: False\n",
            "13:03:31 |     allow_missing_init_opts: False\n",
            "13:03:31 |     attention: dot\n",
            "13:03:31 |     attention_length: 48\n",
            "13:03:31 |     attention_time: post\n",
            "13:03:31 |     batchsize: 1\n",
            "13:03:31 |     beam_block_full_context: True\n",
            "13:03:31 |     beam_block_list_filename: None\n",
            "13:03:31 |     beam_block_ngram: -1\n",
            "13:03:31 |     beam_context_block_ngram: -1\n",
            "13:03:31 |     beam_delay: 30\n",
            "13:03:31 |     beam_length_penalty: 0.65\n",
            "13:03:31 |     beam_min_length: 1\n",
            "13:03:31 |     beam_size: 1\n",
            "13:03:31 |     betas: '(0.9, 0.999)'\n",
            "13:03:31 |     bidirectional: False\n",
            "13:03:31 |     bpe_add_prefix_space: None\n",
            "13:03:31 |     bpe_debug: False\n",
            "13:03:31 |     bpe_merge: None\n",
            "13:03:31 |     bpe_vocab: None\n",
            "13:03:31 |     compute_tokenized_bleu: False\n",
            "13:03:31 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:03:31 |     datatype: train\n",
            "13:03:31 |     decoder: same\n",
            "13:03:31 |     delimiter: '\\n'\n",
            "13:03:31 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:03:31 |     dict_endtoken: __end__\n",
            "13:03:31 |     dict_file: from_scratch_model/model.dict\n",
            "13:03:31 |     dict_include_test: False\n",
            "13:03:31 |     dict_include_valid: False\n",
            "13:03:31 |     dict_initpath: None\n",
            "13:03:31 |     dict_language: english\n",
            "13:03:31 |     dict_loaded: False\n",
            "13:03:31 |     dict_lower: False\n",
            "13:03:31 |     dict_max_ngram_size: -1\n",
            "13:03:31 |     dict_maxexs: -1\n",
            "13:03:31 |     dict_maxtokens: -1\n",
            "13:03:31 |     dict_minfreq: 0\n",
            "13:03:31 |     dict_nulltoken: __null__\n",
            "13:03:31 |     dict_starttoken: __start__\n",
            "13:03:31 |     dict_textfields: text,labels\n",
            "13:03:31 |     dict_tokenizer: re\n",
            "13:03:31 |     dict_unktoken: __unk__\n",
            "13:03:31 |     display_examples: False\n",
            "13:03:31 |     download_path: None\n",
            "13:03:31 |     dropout: 0.1\n",
            "13:03:31 |     dynamic_batching: None\n",
            "13:03:31 |     embedding_projection: random\n",
            "13:03:31 |     embedding_type: random\n",
            "13:03:31 |     embeddingsize: 128\n",
            "13:03:31 |     eval_batchsize: None\n",
            "13:03:31 |     evaltask: None\n",
            "13:03:31 |     force_fp16_tokens: False\n",
            "13:03:31 |     fp16: False\n",
            "13:03:31 |     fp16_impl: apex\n",
            "13:03:31 |     gpu: -1\n",
            "13:03:31 |     gradient_clip: 0.1\n",
            "13:03:31 |     hf_skip_special_tokens: True\n",
            "13:03:31 |     hiddensize: 128\n",
            "13:03:31 |     hide_labels: False\n",
            "13:03:31 |     history_add_global_end_token: None\n",
            "13:03:31 |     history_reversed: False\n",
            "13:03:31 |     history_size: -1\n",
            "13:03:31 |     image_cropsize: 224\n",
            "13:03:31 |     image_mode: no_image_model\n",
            "13:03:31 |     image_size: 256\n",
            "13:03:31 |     inference: greedy\n",
            "13:03:31 |     init_model: None\n",
            "13:03:31 |     init_opt: None\n",
            "13:03:31 |     input_dropout: 0.0\n",
            "13:03:31 |     interactive_mode: False\n",
            "13:03:31 |     invsqrt_lr_decay_gamma: -1\n",
            "13:03:31 |     label_truncate: None\n",
            "13:03:31 |     learningrate: 1\n",
            "13:03:31 |     load_from_checkpoint: True\n",
            "13:03:31 |     log_every_n_secs: 10\n",
            "13:03:31 |     loglevel: info\n",
            "13:03:31 |     lookuptable: all\n",
            "13:03:31 |     lr_scheduler: reduceonplateau\n",
            "13:03:31 |     lr_scheduler_decay: 0.5\n",
            "13:03:31 |     lr_scheduler_patience: 3\n",
            "13:03:31 |     max_lr_steps: -1\n",
            "13:03:31 |     max_train_time: 120.0\n",
            "13:03:31 |     metrics: default\n",
            "13:03:31 |     model: seq2seq\n",
            "13:03:31 |     model_file: from_scratch_model/model\n",
            "13:03:31 |     momentum: 0\n",
            "13:03:31 |     multitask_weights: [1]\n",
            "13:03:31 |     nesterov: True\n",
            "13:03:31 |     no_cuda: False\n",
            "13:03:31 |     num_epochs: -1\n",
            "13:03:31 |     numlayers: 2\n",
            "13:03:31 |     numsoftmax: 1\n",
            "13:03:31 |     nus: (0.7,)\n",
            "13:03:31 |     optimizer: sgd\n",
            "13:03:31 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "13:03:31 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:03:31 |     person_tokens: False\n",
            "13:03:31 |     rank_candidates: False\n",
            "13:03:31 |     remove_political_convos: False\n",
            "13:03:31 |     rnn_class: lstm\n",
            "13:03:31 |     save_after_valid: False\n",
            "13:03:31 |     save_every_n_secs: -1\n",
            "13:03:31 |     short_final_eval: False\n",
            "13:03:31 |     skip_generation: False\n",
            "13:03:31 |     special_tok_lst: None\n",
            "13:03:31 |     split_lines: False\n",
            "13:03:31 |     starttime: Feb07_13-03\n",
            "13:03:31 |     task: empathetic_dialogues\n",
            "13:03:31 |     temperature: 1.0\n",
            "13:03:31 |     tensorboard_log: False\n",
            "13:03:31 |     tensorboard_logdir: None\n",
            "13:03:31 |     text_truncate: None\n",
            "13:03:31 |     topk: 10\n",
            "13:03:31 |     topp: 0.9\n",
            "13:03:31 |     train_experiencer_only: False\n",
            "13:03:31 |     truncate: 64\n",
            "13:03:31 |     update_freq: 1\n",
            "13:03:31 |     use_reply: label\n",
            "13:03:31 |     validation_cutoff: 1.0\n",
            "13:03:31 |     validation_every_n_epochs: -1\n",
            "13:03:31 |     validation_every_n_secs: -1\n",
            "13:03:31 |     validation_max_exs: -1\n",
            "13:03:31 |     validation_metric: accuracy\n",
            "13:03:31 |     validation_metric_mode: None\n",
            "13:03:31 |     validation_patience: 10\n",
            "13:03:31 |     validation_share_agent: False\n",
            "13:03:31 |     warmup_rate: 0.0001\n",
            "13:03:31 |     warmup_updates: -1\n",
            "13:03:31 |     weight_decay: None\n",
            "13:03:31 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:02<00:00, 28.0kex/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:03:34 | Saving dictionary to from_scratch_model/model.dict\n",
            "13:03:34 | dictionary built with 22419 tokens in 0.0s\n",
            "13:03:34 | No model with opt yet at: from_scratch_model/model(.opt)\n",
            "13:03:34 | Using CUDA\n",
            "13:03:34 | loading dictionary from from_scratch_model/model.dict\n",
            "13:03:34 | num words = 22419\n",
            "13:03:34 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "13:03:34 | Opt:\n",
            "13:03:34 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:03:34 |     adam_eps: 1e-08\n",
            "13:03:34 |     add_p1_after_newln: False\n",
            "13:03:34 |     aggregate_micro: False\n",
            "13:03:34 |     allow_missing_init_opts: False\n",
            "13:03:34 |     attention: dot\n",
            "13:03:34 |     attention_length: 48\n",
            "13:03:34 |     attention_time: post\n",
            "13:03:34 |     batchsize: 16\n",
            "13:03:34 |     beam_block_full_context: True\n",
            "13:03:34 |     beam_block_list_filename: None\n",
            "13:03:34 |     beam_block_ngram: -1\n",
            "13:03:34 |     beam_context_block_ngram: -1\n",
            "13:03:34 |     beam_delay: 30\n",
            "13:03:34 |     beam_length_penalty: 0.65\n",
            "13:03:34 |     beam_min_length: 1\n",
            "13:03:34 |     beam_size: 1\n",
            "13:03:34 |     betas: '(0.9, 0.999)'\n",
            "13:03:34 |     bidirectional: False\n",
            "13:03:34 |     bpe_add_prefix_space: None\n",
            "13:03:34 |     bpe_debug: False\n",
            "13:03:34 |     bpe_merge: None\n",
            "13:03:34 |     bpe_vocab: None\n",
            "13:03:34 |     compute_tokenized_bleu: False\n",
            "13:03:34 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:03:34 |     datatype: train\n",
            "13:03:34 |     decoder: same\n",
            "13:03:34 |     delimiter: '\\n'\n",
            "13:03:34 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:03:34 |     dict_endtoken: __end__\n",
            "13:03:34 |     dict_file: from_scratch_model/model.dict\n",
            "13:03:34 |     dict_include_test: False\n",
            "13:03:34 |     dict_include_valid: False\n",
            "13:03:34 |     dict_initpath: None\n",
            "13:03:34 |     dict_language: english\n",
            "13:03:34 |     dict_loaded: True\n",
            "13:03:34 |     dict_lower: False\n",
            "13:03:34 |     dict_max_ngram_size: -1\n",
            "13:03:34 |     dict_maxexs: -1\n",
            "13:03:34 |     dict_maxtokens: -1\n",
            "13:03:34 |     dict_minfreq: 0\n",
            "13:03:34 |     dict_nulltoken: __null__\n",
            "13:03:34 |     dict_starttoken: __start__\n",
            "13:03:34 |     dict_textfields: text,labels\n",
            "13:03:34 |     dict_tokenizer: re\n",
            "13:03:34 |     dict_unktoken: __unk__\n",
            "13:03:34 |     display_examples: False\n",
            "13:03:34 |     download_path: None\n",
            "13:03:34 |     dropout: 0.1\n",
            "13:03:34 |     dynamic_batching: None\n",
            "13:03:34 |     embedding_projection: random\n",
            "13:03:34 |     embedding_type: random\n",
            "13:03:34 |     embeddingsize: 128\n",
            "13:03:34 |     eval_batchsize: None\n",
            "13:03:34 |     evaltask: None\n",
            "13:03:34 |     force_fp16_tokens: False\n",
            "13:03:34 |     fp16: False\n",
            "13:03:34 |     fp16_impl: apex\n",
            "13:03:34 |     gpu: -1\n",
            "13:03:34 |     gradient_clip: 0.1\n",
            "13:03:34 |     hf_skip_special_tokens: True\n",
            "13:03:34 |     hiddensize: 128\n",
            "13:03:34 |     hide_labels: False\n",
            "13:03:34 |     history_add_global_end_token: None\n",
            "13:03:34 |     history_reversed: False\n",
            "13:03:34 |     history_size: -1\n",
            "13:03:34 |     image_cropsize: 224\n",
            "13:03:34 |     image_mode: raw\n",
            "13:03:34 |     image_size: 256\n",
            "13:03:34 |     inference: greedy\n",
            "13:03:34 |     init_model: None\n",
            "13:03:34 |     init_opt: None\n",
            "13:03:34 |     input_dropout: 0.0\n",
            "13:03:34 |     interactive_mode: False\n",
            "13:03:34 |     invsqrt_lr_decay_gamma: -1\n",
            "13:03:34 |     label_truncate: None\n",
            "13:03:34 |     learningrate: 1\n",
            "13:03:34 |     load_from_checkpoint: True\n",
            "13:03:34 |     log_every_n_secs: 10\n",
            "13:03:34 |     loglevel: info\n",
            "13:03:34 |     lookuptable: all\n",
            "13:03:34 |     lr_scheduler: reduceonplateau\n",
            "13:03:34 |     lr_scheduler_decay: 0.5\n",
            "13:03:34 |     lr_scheduler_patience: 3\n",
            "13:03:34 |     max_lr_steps: -1\n",
            "13:03:34 |     max_train_time: 120.0\n",
            "13:03:34 |     metrics: default\n",
            "13:03:34 |     model: seq2seq\n",
            "13:03:34 |     model_file: from_scratch_model/model\n",
            "13:03:34 |     momentum: 0\n",
            "13:03:34 |     multitask_weights: [1]\n",
            "13:03:34 |     nesterov: True\n",
            "13:03:34 |     no_cuda: False\n",
            "13:03:34 |     num_epochs: -1\n",
            "13:03:34 |     numlayers: 2\n",
            "13:03:34 |     numsoftmax: 1\n",
            "13:03:34 |     nus: (0.7,)\n",
            "13:03:34 |     optimizer: sgd\n",
            "13:03:34 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "13:03:34 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:03:34 |     person_tokens: False\n",
            "13:03:34 |     rank_candidates: False\n",
            "13:03:34 |     remove_political_convos: False\n",
            "13:03:34 |     rnn_class: lstm\n",
            "13:03:34 |     save_after_valid: False\n",
            "13:03:34 |     save_every_n_secs: -1\n",
            "13:03:34 |     short_final_eval: False\n",
            "13:03:34 |     skip_generation: False\n",
            "13:03:34 |     special_tok_lst: None\n",
            "13:03:34 |     split_lines: False\n",
            "13:03:34 |     starttime: Feb07_13-03\n",
            "13:03:34 |     task: empathetic_dialogues\n",
            "13:03:34 |     temperature: 1.0\n",
            "13:03:34 |     tensorboard_log: False\n",
            "13:03:34 |     tensorboard_logdir: None\n",
            "13:03:34 |     text_truncate: None\n",
            "13:03:34 |     topk: 10\n",
            "13:03:34 |     topp: 0.9\n",
            "13:03:34 |     train_experiencer_only: False\n",
            "13:03:34 |     truncate: 64\n",
            "13:03:34 |     update_freq: 1\n",
            "13:03:34 |     use_reply: label\n",
            "13:03:34 |     validation_cutoff: 1.0\n",
            "13:03:34 |     validation_every_n_epochs: -1\n",
            "13:03:34 |     validation_every_n_secs: -1\n",
            "13:03:34 |     validation_max_exs: -1\n",
            "13:03:34 |     validation_metric: accuracy\n",
            "13:03:34 |     validation_metric_mode: None\n",
            "13:03:34 |     validation_patience: 10\n",
            "13:03:34 |     validation_share_agent: False\n",
            "13:03:34 |     warmup_rate: 0.0001\n",
            "13:03:34 |     warmup_updates: -1\n",
            "13:03:34 |     weight_decay: None\n",
            "13:03:35 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "13:03:35 | training...\n",
            "13:03:45 | time:10s total_exs:1824 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 458.4  5192 181.2 1824  .9971   .02881 9.058   1 258.7  2931 8591     .08249                  114 717.1 8123 11.33\n",
            "\n",
            "13:03:55 | time:20s total_exs:3728 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 448.3  5298 189.1 1904  1.105   .02621 8.508   1 254.8  3012 4957      .1181                  233 703.1 8310 11.82\n",
            "\n",
            "13:04:06 | time:30s total_exs:5680 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb  tps   ups\n",
            "       1 445.4  5431 195.1 1952  1.102   .02621 8.199   1 258.6  3152 3638      .1367                  355  704 8583 12.19\n",
            "\n",
            "13:04:16 | time:40s total_exs:7552 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 451.9  5255 186.1 1872  1.101   .02622   7.9   1 263.9  3069 2698      .1484                  472 715.8 8324 11.63\n",
            "\n",
            "13:04:26 | time:50s total_exs:9424 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1   450  5230   186 1872  1.145   .02622 7.723   1 262.5  3052 2261      .1576                  589 712.5 8282 11.62\n",
            "\n",
            "13:04:36 | time:60s total_exs:11360 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 443.2  5328 192.3 1936  1.189   .02622  7.54   1 255.9  3076 1881      .1632                  710 699.1 8404 12.02\n",
            "\n",
            "13:04:46 | time:71s total_exs:13344 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1   455  5606 197.1 1984  1.232   .02622 7.385   1 251.2  3096 1611      .1679                  834 706.2 8701 12.32\n",
            "\n",
            "13:04:56 | time:81s total_exs:15184 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 453.3  5208 183.8 1840  1.215   .02622 7.306   1 257.6  2959 1489      .1700                  949 710.9 8167 11.49\n",
            "\n",
            "13:05:06 | time:91s total_exs:17040 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 457.8  5275 184.4 1856  1.239   .02622  7.15   1 263.5  3036 1274      .1750                 1065 721.3 8311 11.52\n",
            "\n",
            "13:05:16 | time:101s total_exs:18800 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "       1 457.8  5001 174.7 1760  1.265   .02623 7.082   1 262.3  2865 1191      .1791                 1175 720.1 7866 10.92\n",
            "\n",
            "13:05:26 | time:111s total_exs:20672 epochs:0.32\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb  tps   ups\n",
            "       1 465.7  5433 186.7 1872  1.272   .02622  6.99   1 258.3  3014 1086      .1859                 1292  724 8447 11.67\n",
            "\n",
            "13:05:35 | max_train_time elapsed:120.04108381271362s\n",
            "13:05:35 | Using CUDA\n",
            "13:05:35 | loading dictionary from from_scratch_model/model.dict\n",
            "13:05:35 | num words = 22419\n",
            "13:05:35 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "13:05:35 | Loading existing model params from from_scratch_model/model\n",
            "13:05:35 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "13:05:36 | running eval: valid\n",
            "13:06:31 | eval completed in 55.01s\n",
            "13:06:31 | \u001b[1mvalid:\n",
            "    accuracy    bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  \\\n",
            "           0 4.798e-05 572.6  3748 104.3 5738 .1292 .0009806 6.636   1 249.1  1631 761.8      .2078                 1398   \n",
            "     tpb  tps  \n",
            "   821.7 5379\n",
            "\u001b[0m\n",
            "13:06:31 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "13:06:31 | running eval: test\n",
            "13:07:23 | eval completed in 51.38s\n",
            "13:07:23 | \u001b[1mtest:\n",
            "    accuracy    bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  \\\n",
            "           0 1.242e-06 604.5  3872 102.4 5259 .1272 .0009466 6.643   1 252.6  1618 767.6      .2077                 1398   \n",
            "     tpb  tps  \n",
            "   857.1 5490\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(4.798e-05),\n",
              "  'ctpb': GlobalAverageMetric(572.6),\n",
              "  'ctps': GlobalTimerMetric(3748),\n",
              "  'exps': GlobalTimerMetric(104.3),\n",
              "  'exs': SumMetric(5738),\n",
              "  'f1': F1Metric(0.1292),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009806),\n",
              "  'loss': AverageMetric(6.636),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(249.1),\n",
              "  'ltps': GlobalTimerMetric(1631),\n",
              "  'ppl': PPLMetric(761.8),\n",
              "  'token_acc': AverageMetric(0.2078),\n",
              "  'total_train_updates': GlobalFixedMetric(1398),\n",
              "  'tpb': GlobalAverageMetric(821.7),\n",
              "  'tps': GlobalTimerMetric(5379)},\n",
              " {'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(1.242e-06),\n",
              "  'ctpb': GlobalAverageMetric(604.5),\n",
              "  'ctps': GlobalTimerMetric(3872),\n",
              "  'exps': GlobalTimerMetric(102.4),\n",
              "  'exs': SumMetric(5259),\n",
              "  'f1': F1Metric(0.1272),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009466),\n",
              "  'loss': AverageMetric(6.643),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(252.6),\n",
              "  'ltps': GlobalTimerMetric(1618),\n",
              "  'ppl': PPLMetric(767.6),\n",
              "  'token_acc': AverageMetric(0.2077),\n",
              "  'total_train_updates': GlobalFixedMetric(1398),\n",
              "  'tpb': GlobalAverageMetric(857.1),\n",
              "  'tps': GlobalTimerMetric(5490)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA77Zwkoviq"
      },
      "source": [
        "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTiTn7aoxv9"
      },
      "source": [
        "## Performance is pretty bad there. Can we improve it?\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Jt9bHTn1dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87510096-f460-44ae-ce4e-84c0b469cf3b"
      },
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    # similar to before\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    max_train_time=600, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:12:34 | building dictionary first...\n",
            "13:12:34 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "13:12:34 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,tensorboard_logdir: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\u001b[0m\n",
            "13:12:34 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "13:12:34 | Using CUDA\n",
            "13:12:34 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "13:12:34 | num words = 54944\n",
            "13:12:35 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:12:35 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "13:12:36 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "13:12:36 | Opt:\n",
            "13:12:36 |     activation: gelu\n",
            "13:12:36 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:12:36 |     adam_eps: 1e-08\n",
            "13:12:36 |     add_p1_after_newln: False\n",
            "13:12:36 |     aggregate_micro: False\n",
            "13:12:36 |     allow_missing_init_opts: False\n",
            "13:12:36 |     attention_dropout: 0.0\n",
            "13:12:36 |     batchsize: 12\n",
            "13:12:36 |     beam_block_full_context: True\n",
            "13:12:36 |     beam_block_list_filename: None\n",
            "13:12:36 |     beam_block_ngram: -1\n",
            "13:12:36 |     beam_context_block_ngram: -1\n",
            "13:12:36 |     beam_delay: 30\n",
            "13:12:36 |     beam_length_penalty: 0.65\n",
            "13:12:36 |     beam_min_length: 1\n",
            "13:12:36 |     beam_size: 1\n",
            "13:12:36 |     betas: '(0.9, 0.999)'\n",
            "13:12:36 |     bpe_add_prefix_space: None\n",
            "13:12:36 |     bpe_debug: False\n",
            "13:12:36 |     bpe_merge: None\n",
            "13:12:36 |     bpe_vocab: None\n",
            "13:12:36 |     compute_tokenized_bleu: False\n",
            "13:12:36 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:12:36 |     datatype: train\n",
            "13:12:36 |     delimiter: '\\n'\n",
            "13:12:36 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:12:36 |     dict_endtoken: __end__\n",
            "13:12:36 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "13:12:36 |     dict_include_test: False\n",
            "13:12:36 |     dict_include_valid: False\n",
            "13:12:36 |     dict_initpath: None\n",
            "13:12:36 |     dict_language: english\n",
            "13:12:36 |     dict_loaded: True\n",
            "13:12:36 |     dict_lower: True\n",
            "13:12:36 |     dict_max_ngram_size: -1\n",
            "13:12:36 |     dict_maxexs: -1\n",
            "13:12:36 |     dict_maxtokens: -1\n",
            "13:12:36 |     dict_minfreq: 0\n",
            "13:12:36 |     dict_nulltoken: __null__\n",
            "13:12:36 |     dict_starttoken: __start__\n",
            "13:12:36 |     dict_textfields: text,labels\n",
            "13:12:36 |     dict_tokenizer: bpe\n",
            "13:12:36 |     dict_unktoken: __unk__\n",
            "13:12:36 |     display_examples: False\n",
            "13:12:36 |     download_path: None\n",
            "13:12:36 |     dropout: 0.0\n",
            "13:12:36 |     dynamic_batching: full\n",
            "13:12:36 |     embedding_projection: random\n",
            "13:12:36 |     embedding_size: 512\n",
            "13:12:36 |     embedding_type: random\n",
            "13:12:36 |     embeddings_scale: True\n",
            "13:12:36 |     eval_batchsize: None\n",
            "13:12:36 |     evaltask: None\n",
            "13:12:36 |     ffn_size: 2048\n",
            "13:12:36 |     force_fp16_tokens: False\n",
            "13:12:36 |     fp16: True\n",
            "13:12:36 |     fp16_impl: mem_efficient\n",
            "13:12:36 |     gpu: -1\n",
            "13:12:36 |     gradient_clip: 0.1\n",
            "13:12:36 |     hf_skip_special_tokens: True\n",
            "13:12:36 |     hide_labels: False\n",
            "13:12:36 |     history_add_global_end_token: None\n",
            "13:12:36 |     history_reversed: False\n",
            "13:12:36 |     history_size: -1\n",
            "13:12:36 |     image_cropsize: 224\n",
            "13:12:36 |     image_mode: raw\n",
            "13:12:36 |     image_size: 256\n",
            "13:12:36 |     inference: greedy\n",
            "13:12:36 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "13:12:36 |     init_opt: None\n",
            "13:12:36 |     interactive_mode: False\n",
            "13:12:36 |     invsqrt_lr_decay_gamma: -1\n",
            "13:12:36 |     label_truncate: 128\n",
            "13:12:36 |     learn_positional_embeddings: True\n",
            "13:12:36 |     learningrate: 1e-05\n",
            "13:12:36 |     load_from_checkpoint: True\n",
            "13:12:36 |     log_every_n_secs: 10\n",
            "13:12:36 |     loglevel: info\n",
            "13:12:36 |     lr_scheduler: reduceonplateau\n",
            "13:12:36 |     lr_scheduler_decay: 0.5\n",
            "13:12:36 |     lr_scheduler_patience: 3\n",
            "13:12:36 |     max_lr_steps: -1\n",
            "13:12:36 |     max_train_time: 600.0\n",
            "13:12:36 |     metrics: default\n",
            "13:12:36 |     model: transformer/generator\n",
            "13:12:36 |     model_file: from_pretrained/model\n",
            "13:12:36 |     model_parallel: False\n",
            "13:12:36 |     momentum: 0\n",
            "13:12:36 |     multitask_weights: [1]\n",
            "13:12:36 |     n_decoder_layers: -1\n",
            "13:12:36 |     n_encoder_layers: -1\n",
            "13:12:36 |     n_heads: 16\n",
            "13:12:36 |     n_layers: 8\n",
            "13:12:36 |     n_positions: 512\n",
            "13:12:36 |     n_segments: 0\n",
            "13:12:36 |     nesterov: True\n",
            "13:12:36 |     no_cuda: False\n",
            "13:12:36 |     num_epochs: -1\n",
            "13:12:36 |     nus: (0.7,)\n",
            "13:12:36 |     optimizer: mem_eff_adam\n",
            "13:12:36 |     output_scaling: 1.0\n",
            "13:12:36 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "13:12:36 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:12:36 |     person_tokens: False\n",
            "13:12:36 |     rank_candidates: False\n",
            "13:12:36 |     relu_dropout: 0.0\n",
            "13:12:36 |     remove_political_convos: False\n",
            "13:12:36 |     save_after_valid: False\n",
            "13:12:36 |     save_every_n_secs: -1\n",
            "13:12:36 |     share_word_embeddings: True\n",
            "13:12:36 |     short_final_eval: False\n",
            "13:12:36 |     skip_generation: True\n",
            "13:12:36 |     special_tok_lst: None\n",
            "13:12:36 |     split_lines: False\n",
            "13:12:36 |     starttime: Feb07_13-12\n",
            "13:12:36 |     task: empathetic_dialogues\n",
            "13:12:36 |     temperature: 1.0\n",
            "13:12:36 |     tensorboard_log: False\n",
            "13:12:36 |     tensorboard_logdir: None\n",
            "13:12:36 |     text_truncate: 512\n",
            "13:12:36 |     topk: 10\n",
            "13:12:36 |     topp: 0.9\n",
            "13:12:36 |     train_experiencer_only: False\n",
            "13:12:36 |     truncate: -1\n",
            "13:12:36 |     update_freq: 1\n",
            "13:12:36 |     use_reply: label\n",
            "13:12:36 |     validation_cutoff: 1.0\n",
            "13:12:36 |     validation_every_n_epochs: 0.25\n",
            "13:12:36 |     validation_every_n_secs: -1\n",
            "13:12:36 |     validation_max_exs: -1\n",
            "13:12:36 |     validation_metric: ppl\n",
            "13:12:36 |     validation_metric_mode: None\n",
            "13:12:36 |     validation_patience: 10\n",
            "13:12:36 |     validation_share_agent: False\n",
            "13:12:36 |     variant: xlm\n",
            "13:12:36 |     warmup_rate: 0.0001\n",
            "13:12:36 |     warmup_updates: 100\n",
            "13:12:36 |     weight_decay: None\n",
            "13:12:37 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "13:12:41 | training...\n",
            "13:12:42 | Overflow: setting loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/parlai/utils/fp16.py:487: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:12:47 | Overflow: setting loss scale to 32768.0\n",
            "13:12:50 | Overflow: setting loss scale to 16384.0\n",
            "13:12:51 | time:10s total_exs:3200 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9000  2627  8088 328.4 3200             48606  3.767    .5154 2.844 3.001e-06  1716  5284 17.18      .3946   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                     30 4342 13372 3.08\n",
            "\n",
            "13:13:01 | time:20s total_exs:6296 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3100  9670 301.8 3096             16384  4.167    .5105 2.811 6.2e-06  1634  5096 16.63      .4004   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     62 4734 14766 3.119\n",
            "\n",
            "13:13:12 | time:31s total_exs:9416 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3119  9844 307.7 3120             16384  4.483    .4954 2.761 9.4e-06  1580  4987 15.82      .4049   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     94 4699 14831 3.156\n",
            "\n",
            "13:13:22 | time:41s total_exs:12588 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2937  9043   315 3172             16384  3.981    .5105 2.764 1e-05  1694  5215 15.86      .4036   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    125 4631 14258 3.079\n",
            "\n",
            "13:13:32 | time:51s total_exs:15580 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3102  9658 291.1 2992             16384  4.013    .4895 2.734 1e-05  1617  5035 15.4      .4053   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    157 4719 14693 3.114\n",
            "\n",
            "13:13:34 | time:54s total_exs:16296 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2905  9328 287.4  716             16384  4.412    .5050 2.686 1e-05  1408  4521 14.68      .4122   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    165 4313 13850 3.212\n",
            "\n",
            "13:13:34 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "13:13:36 | running eval: valid\n",
            "13:13:37 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "13:13:43 | eval completed in 6.89s\n",
            "13:13:43 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 35512 877.9 5738   .08918 2.534 1e-05  1351 14056 12.6      .4304                  165 4764 49568\n",
            "\u001b[0m\n",
            "13:13:43 | \u001b[1;32mnew best ppl: 12.6\u001b[0m\n",
            "13:13:43 | saving best valid model: from_pretrained/model\n",
            "13:13:43 | Saving dictionary to from_pretrained/model.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:13:58 | time:77s total_exs:19576 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3111  9679 318.8 3280             16384  4.148    .4954 2.693 1e-05  1629  5067 14.78      .4122   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    197 4741 14746 3.111\n",
            "\n",
            "13:14:08 | time:87s total_exs:22604 epochs:0.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3014  9501 298.2 3028             16384  4.057    .4841 2.716 1e-05  1653  5211 15.12      .4095   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    229 4668 14712 3.152\n",
            "\n",
            "13:14:18 | time:98s total_exs:25640 epochs:0.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2921  9168 297.8 3036             16384  4.417    .5422 2.694 1e-05  1533  4811 14.79      .4144   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    261 4453 13979 3.139\n",
            "\n",
            "13:14:29 | time:108s total_exs:28760 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2778  9133 301.6 3120             16384  4.253    .5190 2.698 1e-05  1572  5167 14.85      .4145   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    295 4350 14300 3.287\n",
            "\n",
            "13:14:39 | time:118s total_exs:31676 epochs:0.49\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3037  9694 290.8 2916             16384   4.18    .5004 2.664 1e-05  1511  4823 14.35      .4168   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    327 4548 14517 3.192\n",
            "\n",
            "13:14:42 | time:121s total_exs:32544 epochs:0.50\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3167 10011 274.3  868             16384  4.642    .4760 2.682 1e-05  1551  4902 14.61      .4130   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    337 4718 14914 3.162\n",
            "\n",
            "13:14:42 | running eval: valid\n",
            "13:14:49 | eval completed in 6.46s\n",
            "13:14:49 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 36873 911.5 5738   .08918 2.504 1e-05  1371 14594 12.23      .4366                  337 4835 51467\n",
            "\u001b[0m\n",
            "13:14:49 | \u001b[1;32mnew best ppl: 12.23 (previous best was 12.6)\u001b[0m\n",
            "13:14:49 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:15:03 | time:142s total_exs:35556 epochs:0.55\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3093  9168 297.6 3012             16384  3.898    .5154 2.663 1e-05  1683  4989 14.34      .4167   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    367 4775 14157 2.965\n",
            "\n",
            "13:15:14 | time:153s total_exs:38632 epochs:0.60\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2966  9232 299.2 3076             16384   4.32    .5050 2.664 1e-05  1586  4935 14.35      .4155   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    399 4552 14168 3.112\n",
            "\n",
            "13:15:24 | time:163s total_exs:41700 epochs:0.65\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2613  8238 302.3 3068             16384  4.133    .5238 2.673 1e-05  1573  4958 14.49      .4148   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    431 4185 13196 3.153\n",
            "\n",
            "13:15:34 | time:173s total_exs:44404 epochs:0.69\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2962  8946 263.5 2704             16384  4.435    .4977 2.686 1e-05  1516  4580 14.67      .4135   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    462 4478 13525 3.021\n",
            "\n",
            "13:15:44 | time:183s total_exs:47220 epochs:0.73\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3237 10185 276.8 2816             16384  4.716    .5105 2.674 1e-05  1441  4532 14.5      .4144   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    494 4678 14717 3.146\n",
            "\n",
            "13:15:49 | time:188s total_exs:48716 epochs:0.75\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3045  9688 297.4 1496             16384   4.01    .5233 2.611 1e-05  1478  4704 13.61      .4237   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    510 4524 14392 3.182\n",
            "\n",
            "13:15:49 | running eval: valid\n",
            "13:15:56 | eval completed in 6.48s\n",
            "13:15:56 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 36878 911.6 5738   .08918 2.488 1e-05  1371 14596 12.04      .4383                  510 4835 51475\n",
            "\u001b[0m\n",
            "13:15:56 | \u001b[1;32mnew best ppl: 12.04 (previous best was 12.23)\u001b[0m\n",
            "13:15:56 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:16:11 | time:210s total_exs:51648 epochs:0.80\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2722  8253 286.7 2932             16384   4.08    .5237 2.663 1e-05  1667  5053 14.34      .4140   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    541 4389 13307 3.032\n",
            "\n",
            "13:16:21 | time:220s total_exs:54516 epochs:0.84\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2967  9340 282.2 2868             16384  4.283    .4954 2.656 1e-05  1509  4752 14.25      .4165   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    573 4476 14092 3.149\n",
            "\n",
            "13:16:31 | time:230s total_exs:57472 epochs:0.89\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3057  9445 294.6 2956             16384  4.188    .5190 2.628 1e-05  1544  4771 13.85      .4230   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    604 4601 14216 3.09\n",
            "\n",
            "13:16:41 | time:240s total_exs:60808 epochs:0.94\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2831  8912 328.1 3336             16384  3.999    .5050 2.644 1e-05  1772  5576 14.07      .4169   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    636 4603 14488 3.148\n",
            "\n",
            "13:16:51 | time:250s total_exs:63836 epochs:0.99\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3215  9976 303.1 3028             16384  4.289    .5105 2.603 1e-05  1583  4911 13.5      .4235   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    667 4797 14887 3.103\n",
            "\n",
            "13:16:55 | time:254s total_exs:64876 epochs:1.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2939  9021 290.2 1040             16384  4.667    .4925 2.626 1e-05  1435  4405 13.81      .4246   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    678 4374 13427 3.07\n",
            "\n",
            "13:16:55 | running eval: valid\n",
            "13:17:01 | eval completed in 6.41s\n",
            "13:17:01 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37638 930.3 5738   .08918 2.478 1e-05  1392 14897 11.91      .4404                  678 4909 52536\n",
            "\u001b[0m\n",
            "13:17:01 | \u001b[1;32mnew best ppl: 11.91 (previous best was 12.04)\u001b[0m\n",
            "13:17:01 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:17:16 | time:275s total_exs:67624 epochs:1.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3077  9439 271.9 2748             16384  4.161    .5105 2.633 1e-05  1573  4825 13.91      .4191   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    709 4650 14264 3.067\n",
            "\n",
            "13:17:26 | time:285s total_exs:70792 epochs:1.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3016  9199 311.7 3168             16384  4.327    .4954 2.608 1e-05  1682  5131 13.57      .4235   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    740 4698 14330 3.05\n",
            "\n",
            "13:17:37 | time:296s total_exs:73588 epochs:1.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3097  9642   272 2796             16384  4.358    .4982 2.682 1e-05  1453  4522 14.62      .4153   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    772 4550 14164 3.113\n",
            "\n",
            "13:17:47 | time:306s total_exs:76672 epochs:1.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2954  9005 303.2 3084             16384  4.019    .5154  2.62 1e-05  1683  5130 13.74      .4214   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    803 4638 14136 3.048\n",
            "\n",
            "13:17:57 | time:316s total_exs:79688 epochs:1.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2902  8951   300 3016             16384  4.205    .5105 2.635 1e-05  1611  4969 13.95      .4249   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    834 4514 13920 3.084\n",
            "\n",
            "13:18:03 | time:322s total_exs:81048 epochs:1.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3134  9646 232.5 1360             16384   4.38    .4841 2.622 1e-05  1440  4431 13.76      .4233   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    852 4573 14076 3.078\n",
            "\n",
            "13:18:03 | running eval: valid\n",
            "13:18:09 | eval completed in 6.50s\n",
            "13:18:09 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3571 37953 938.2 5738   .08915 2.468 1e-05  1413 15022 11.8      .4421                  852 4984 52975\n",
            "\u001b[0m\n",
            "13:18:09 | \u001b[1;32mnew best ppl: 11.8 (previous best was 11.91)\u001b[0m\n",
            "13:18:09 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:18:24 | time:343s total_exs:84048 epochs:1.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3026  9261 296.2 3000             16384  4.029    .5232 2.617 1e-05  1609  4923 13.7      .4230   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    883 4634 14185 3.061\n",
            "\n",
            "13:18:34 | time:353s total_exs:87300 epochs:1.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2879  9035 318.9 3252             16384   3.94    .5050 2.628 1e-05  1665  5226 13.84      .4198   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    915 4544 14261 3.139\n",
            "\n",
            "13:18:44 | time:363s total_exs:90128 epochs:1.39\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2962  8800 280.1 2828             16384  4.096    .5105 2.604 1e-05  1558  4628 13.51      .4252   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    945 4520 13428 2.971\n",
            "\n",
            "13:18:55 | time:374s total_exs:92972 epochs:1.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3051  9184 276.1 2844             16384  4.092    .4954 2.635 1e-05  1614  4859 13.94      .4212   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    976 4666 14043 3.01\n",
            "\n",
            "13:19:05 | time:384s total_exs:96036 epochs:1.49\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3095  9806 303.3 3064             16384  4.529    .5105  2.57 1e-05  1510  4784 13.06      .4271   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1008 4605 14589 3.169\n",
            "\n",
            "13:19:09 | time:388s total_exs:97320 epochs:1.51\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2800  8280 316.3 1284             16384  3.777    .4926 2.625 1e-05  1775  5249 13.81      .4207   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1020 4576 13529 2.957\n",
            "\n",
            "13:19:09 | running eval: valid\n",
            "13:19:15 | eval completed in 6.61s\n",
            "13:19:15 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3571 36487   902 5738   .08919 2.463 1e-05  1413 14442 11.74      .4425                 1020 4984 50929\n",
            "\u001b[0m\n",
            "13:19:15 | \u001b[1;32mnew best ppl: 11.74 (previous best was 11.8)\u001b[0m\n",
            "13:19:15 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:19:30 | time:409s total_exs:100352 epochs:1.55\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3141  9701 302.1 3032             16384  4.062    .5233 2.585 1e-05  1634  5047 13.27      .4299   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1051 4774 14748 3.089\n",
            "\n",
            "13:19:40 | time:419s total_exs:103096 epochs:1.60\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3035  9628   272 2744             16384  4.253    .5003 2.571 1e-05  1442  4574 13.07      .4282   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1083 4477 14202 3.172\n",
            "\n",
            "13:19:50 | time:430s total_exs:106132 epochs:1.64\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2538  7704 297.3 3036             16384  4.336    .5322 2.613 1e-05  1694  5143 13.65      .4243   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1114 4232 12847 3.036\n",
            "\n",
            "13:20:01 | time:440s total_exs:109128 epochs:1.69\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2986  9387 294.3 2996             16384  3.992    .4954 2.582 1e-05  1581  4969 13.22      .4276   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1146 4567 14356 3.144\n",
            "\n",
            "13:20:11 | time:450s total_exs:111968 epochs:1.73\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3283 10085 281.4 2840             16384  4.356    .4744 2.571 1e-05  1477  4538 13.08      .4303   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1177 4760 14623 3.072\n",
            "\n",
            "13:20:16 | time:455s total_exs:113612 epochs:1.76\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3316 10371 302.5 1644             16384  4.524    .4895 2.611 1e-05  1539  4814 13.61      .4214   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1194 4855 15185 3.128\n",
            "\n",
            "13:20:16 | running eval: valid\n",
            "13:20:23 | eval completed in 6.31s\n",
            "13:20:23 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38047 940.5 5738   .08921 2.457 1e-05  1392 15059 11.67      .4436                 1194 4909 53106\n",
            "\u001b[0m\n",
            "13:20:23 | \u001b[1;32mnew best ppl: 11.67 (previous best was 11.74)\u001b[0m\n",
            "13:20:23 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:20:38 | time:477s total_exs:116680 epochs:1.81\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2916  9091 298.8 3068             16384  4.167    .5233 2.582 1e-05  1581  4927 13.23      .4249   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1226 4497 14018 3.117\n",
            "\n",
            "13:20:48 | time:487s total_exs:119888 epochs:1.85\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2945  9411 320.3 3208             16384  4.202    .4841 2.609 1e-05  1663  5314 13.58      .4223   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1258 4608 14725 3.196\n",
            "\n",
            "13:20:58 | time:497s total_exs:122860 epochs:1.90\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2776  8492 293.3 2972             16384  4.331    .5105 2.608 1e-05  1605  4909 13.57      .4201   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1289 4380 13402 3.06\n",
            "\n",
            "13:21:08 | time:507s total_exs:125836 epochs:1.95\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3100  9883 296.4 2976             16384  4.362    .5322 2.582 1e-05  1507  4804 13.23      .4252   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1321 4607 14687 3.188\n",
            "\n",
            "13:21:18 | time:517s total_exs:128632 epochs:1.99\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2781  8593 278.6 2796             16384  4.246    .5050 2.596 1e-05  1472  4548 13.41      .4257   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1352 4254 13141 3.09\n",
            "\n",
            "13:21:22 | time:521s total_exs:129780 epochs:2.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2787  8317 285.5 1148             16384  3.991    .5105 2.686 1e-05  1905  5686 14.67      .4140   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1364 4692 14003 2.985\n",
            "\n",
            "13:21:22 | running eval: valid\n",
            "13:21:29 | eval completed in 6.75s\n",
            "13:21:29 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 36475 901.7 5738    .0892 2.453 1e-05  1392 14437 11.63      .4432                 1364 4909 50913\n",
            "\u001b[0m\n",
            "13:21:29 | \u001b[1;32mnew best ppl: 11.63 (previous best was 11.67)\u001b[0m\n",
            "13:21:29 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:21:43 | time:542s total_exs:132696 epochs:2.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3125 10269 290.3 2916             16384  4.297    .4954 2.564 1e-05  1436  4719 12.99      .4314   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1397 4562 14988 3.286\n",
            "\n",
            "13:21:54 | time:553s total_exs:135736 epochs:2.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2895  8960   294 3040             16384  4.092    .5105 2.597 1e-05  1630  5045 13.42      .4218   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1429 4525 14005 3.095\n",
            "\n",
            "13:22:04 | time:563s total_exs:138700 epochs:2.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2994  9514 294.3 2964             16384  4.335    .5190  2.57 1e-05  1531  4864 13.06      .4261   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1461 4524 14378 3.178\n",
            "\n",
            "13:22:14 | time:573s total_exs:141744 epochs:2.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2944  9117   304 3044             16384  4.309    .5322 2.604 1e-05  1706  5283 13.52      .4196   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1492 4650 14400 3.097\n",
            "\n",
            "13:22:24 | time:583s total_exs:144680 epochs:2.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2908  9306 293.6 2936             16384  4.326    .5233 2.571 1e-05  1554  4972 13.07      .4275   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1524 4462 14278  3.2\n",
            "\n",
            "13:22:28 | time:588s total_exs:146020 epochs:2.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3255 10093 296.8 1340             16384  4.098    .4930 2.544 1e-05  1546  4795 12.73      .4389   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1538 4801 14889 3.102\n",
            "\n",
            "13:22:28 | running eval: valid\n",
            "13:22:35 | eval completed in 6.72s\n",
            "13:22:35 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 35727 883.2 5738   .08916  2.45 1e-05  1392 14141 11.58      .4439                 1538 4909 49868\n",
            "\u001b[0m\n",
            "13:22:35 | \u001b[1;32mnew best ppl: 11.58 (previous best was 11.63)\u001b[0m\n",
            "13:22:35 | saving best valid model: from_pretrained/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:22:41 | max_train_time elapsed:600.173770904541s\n",
            "13:22:41 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "13:22:41 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "13:22:41 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,tensorboard_logdir: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages,dict_loaded: True,download_path: None,datapath: /usr/local/lib/python3.6/dist-packages/data,interactive_mode: False\u001b[0m\n",
            "13:22:41 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "13:22:41 | Using CUDA\n",
            "13:22:41 | loading dictionary from from_pretrained/model.dict\n",
            "13:22:41 | num words = 54944\n",
            "13:22:43 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:22:43 | Loading existing model params from from_pretrained/model\n",
            "13:22:46 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "13:22:48 | running eval: valid\n",
            "13:22:55 | eval completed in 6.78s\n",
            "13:22:55 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 35550 878.8 5738   .07776  2.45 1e-05  1351 14071 11.58      .4441                 1538 4764 49620\n",
            "\u001b[0m\n",
            "13:22:55 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "13:22:58 | running eval: test\n",
            "13:23:04 | eval completed in 6.59s\n",
            "13:23:04 | \u001b[1mtest:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3590 36341 831.8 5259   .07778 2.471 1e-05  1334 13501 11.84      .4411                 1538 4923 49842\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'ctpb': GlobalAverageMetric(3413),\n",
              "  'ctps': GlobalTimerMetric(3.555e+04),\n",
              "  'exps': GlobalTimerMetric(878.8),\n",
              "  'exs': SumMetric(5738),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07776),\n",
              "  'loss': AverageMetric(2.45),\n",
              "  'lr': GlobalAverageMetric(1e-05),\n",
              "  'ltpb': GlobalAverageMetric(1351),\n",
              "  'ltps': GlobalTimerMetric(1.407e+04),\n",
              "  'ppl': PPLMetric(11.58),\n",
              "  'token_acc': AverageMetric(0.4441),\n",
              "  'total_train_updates': GlobalFixedMetric(1538),\n",
              "  'tpb': GlobalAverageMetric(4764),\n",
              "  'tps': GlobalTimerMetric(4.962e+04)},\n",
              " {'ctpb': GlobalAverageMetric(3590),\n",
              "  'ctps': GlobalTimerMetric(3.634e+04),\n",
              "  'exps': GlobalTimerMetric(831.8),\n",
              "  'exs': SumMetric(5259),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07778),\n",
              "  'loss': AverageMetric(2.471),\n",
              "  'lr': GlobalAverageMetric(1e-05),\n",
              "  'ltpb': GlobalAverageMetric(1334),\n",
              "  'ltps': GlobalTimerMetric(1.35e+04),\n",
              "  'ppl': PPLMetric(11.84),\n",
              "  'token_acc': AverageMetric(0.4411),\n",
              "  'total_train_updates': GlobalFixedMetric(1538),\n",
              "  'tpb': GlobalAverageMetric(4923),\n",
              "  'tps': GlobalTimerMetric(4.984e+04)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBZXTLRvIjb"
      },
      "source": [
        "## Wow that's a lot of options? Where do I find more info?\n",
        "\n",
        "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
        "\n",
        "You can get some guidance in this notebook by using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pl8VVl5plfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b79a0f-d325-4aef-a98e-5a4ec3915068"
      },
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: TrainModel [-h] [--helpall] [-o INIT_OPT]\n",
            "                  [--allow-missing-init-opts ALLOW_MISSING_INIT_OPTS]\n",
            "                  [-t TASK] [-dt DATATYPE] [-bs BATCHSIZE]\n",
            "                  [-dynb {None,batchsort,full}] [-dp DATAPATH] [-m MODEL]\n",
            "                  [-mf MODEL_FILE] [-im INIT_MODEL] [-et EVALTASK]\n",
            "                  [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME]\n",
            "                  [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
            "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
            "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
            "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
            "                  [-tblog TENSORBOARD_LOG] [-tblogdir TENSORBOARD_LOGDIR]\n",
            "                  [-hs HIDDENSIZE] [-esz EMBEDDINGSIZE] [-nl NUMLAYERS]\n",
            "                  [-dr DROPOUT] [-bi BIDIRECTIONAL]\n",
            "                  [-att {none,concat,general,dot,local}]\n",
            "                  [-attl ATTENTION_LENGTH] [--attention-time {pre,post}]\n",
            "                  [-rnn {rnn,gru,lstm}] [-dec {same,shared}]\n",
            "                  [-lt {unique,enc_dec,dec_out,all}] [-soft NUMSOFTMAX]\n",
            "                  [-idr INPUT_DROPOUT] [--beam-size BEAM_SIZE]\n",
            "                  [--beam-min-length BEAM_MIN_LENGTH]\n",
            "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
            "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
            "                  [--beam-block-full-context BEAM_BLOCK_FULL_CONTEXT]\n",
            "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
            "                  [--inference {greedy,topk,nucleus,delayedbeam,beam}]\n",
            "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
            "                  [--beam-block-list-filename BEAM_BLOCK_LIST_FILENAME]\n",
            "                  [--temperature TEMPERATURE]\n",
            "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
            "                  [-i INTERACTIVE_MODE]\n",
            "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
            "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
            "                  [--fp16-impl {apex,mem_efficient}] [-opt OPTIMIZER]\n",
            "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
            "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
            "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
            "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
            "                  [--text-truncate TEXT_TRUNCATE]\n",
            "                  [--label-truncate LABEL_TRUNCATE]\n",
            "                  [--history-reversed HISTORY_REVERSED] [-histsz HISTORY_SIZE]\n",
            "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
            "                  [--delimiter DELIMITER] [--special-tok-lst SPECIAL_TOK_LST]\n",
            "                  [-gpu GPU | --no-cuda] [--bpe-vocab BPE_VOCAB]\n",
            "                  [--bpe-merge BPE_MERGE]\n",
            "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
            "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
            "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
            "                  [--max-lr-steps MAX_LR_STEPS]\n",
            "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "        show this help message and exit\n",
            "  --helpall\n",
            "        Show usage, including advanced arguments.\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "        Path to json file of options. Note: Further Command-line arguments\n",
            "        override file-based options. (default: None)\n",
            "  --allow-missing-init-opts ALLOW_MISSING_INIT_OPTS\n",
            "        Warn instead of raising if an argument passed in with --init-opt is\n",
            "        not in the target opt. (default: False)\n",
            "  -t, --task TASK\n",
            "        ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype DATATYPE\n",
            "        choose from: train, train:ordered, valid, test. to stream data add\n",
            "        \":stream\" to any option (e.g., train:stream). by default train is\n",
            "        random with replacement, valid is ordered, test is ordered. (default:\n",
            "        train)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "        batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {None,batchsort,full}\n",
            "        Use dynamic batching (default: None)\n",
            "  -dp, --datapath DATAPATH\n",
            "        path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "        the model class name. can match parlai/agents/<model> for agents in\n",
            "        that directory, or can provide a fully specified module for `from X\n",
            "        import Y` via `-m X:Y` (e.g. `-m\n",
            "        parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`) (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "        model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "        Initialize model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "        task to use for valid/test (defaults to the one used for training)\n",
            "        (default: None)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "        Validate every n seconds. Saves model to model_file (if set) whenever\n",
            "        best val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "        Saves the model to model_file.checkpoint after every n seconds\n",
            "        (default -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "        Saves the model to model_file.checkpoint after every validation\n",
            "        (default False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "        Validate every n epochs. Saves model to model_file (if set) whenever\n",
            "        best val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "        number of iterations of validation where result does not improve\n",
            "        before we stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "        key into report table for selecting best validation (default:\n",
            "        accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "        how to optimize validation metric (max or min) (default: None)\n",
            "  -mcs, --metrics METRICS\n",
            "        list of metrics to show/compute, e.g. all, default,or give a list\n",
            "        split by , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics\n",
            "        will be computed as rouge-1, rouge-2 and rouge-l (default: default)\n",
            "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
            "        Report micro-averaged metrics instead of macro averaged metrics.\n",
            "        (default: False)\n",
            "\n",
            "Tensorboard Arguments:\n",
            "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
            "        Tensorboard logging of metrics, default is False\n",
            "  -tblogdir, --tensorboard-logdir TENSORBOARD_LOGDIR\n",
            "        Tensorboard logging directory, defaults to model_file.tensorboard\n",
            "        (default: None)\n",
            "\n",
            "Seq2Seq Arguments:\n",
            "  -hs, --hiddensize HIDDENSIZE\n",
            "        size of the hidden layers (default: 128)\n",
            "  -esz, --embeddingsize EMBEDDINGSIZE\n",
            "        size of the token embeddings (default: 128)\n",
            "  -nl, --numlayers NUMLAYERS\n",
            "        number of hidden layers (default: 2)\n",
            "  -dr, --dropout DROPOUT\n",
            "        dropout rate (default: 0.1)\n",
            "  -bi, --bidirectional BIDIRECTIONAL\n",
            "        whether to encode the context with a bidirectional rnn (default:\n",
            "        False)\n",
            "  -att, --attention {none,concat,general,dot,local}\n",
            "        Choices: none, concat, general, local. If set local, also set\n",
            "        attention-length. (see arxiv.org/abs/1508.04025) (default: none)\n",
            "  -attl, --attention-length ATTENTION_LENGTH\n",
            "        Length of local attention. (default: 48)\n",
            "  --attention-time {pre,post}\n",
            "        Whether to apply attention before or after decoding. (default: post)\n",
            "  -rnn, --rnn-class {rnn,gru,lstm}\n",
            "        Choose between different types of RNNs. (default: lstm)\n",
            "  -dec, --decoder {same,shared}\n",
            "        Choose between different decoder modules. Default \"same\" uses same\n",
            "        class as encoder, while \"shared\" also uses the same weights. Note that\n",
            "        shared disabled some encoder options--in particular, bidirectionality.\n",
            "        (default: same)\n",
            "  -lt, --lookuptable {unique,enc_dec,dec_out,all}\n",
            "        The encoder, decoder, and output modules can share weights, or not.\n",
            "        Unique has independent embeddings for each. Enc_dec shares the\n",
            "        embedding for the encoder and decoder. Dec_out shares decoder\n",
            "        embedding and output weights. All shares all three weights. (default:\n",
            "        unique)\n",
            "  -soft, --numsoftmax NUMSOFTMAX\n",
            "        default 1, if greater then uses mixture of softmax (see\n",
            "        arxiv.org/abs/1711.03953). (default: 1)\n",
            "  -idr, --input-dropout INPUT_DROPOUT\n",
            "        Probability of replacing tokens with UNK in training. (default: 0.0)\n",
            "\n",
            "Torch Generator Agent:\n",
            "  --beam-size BEAM_SIZE\n",
            "        Beam size, if 1 then greedy search (default: 1)\n",
            "  --beam-min-length BEAM_MIN_LENGTH\n",
            "        Minimum length of prediction to be generated by the beam search\n",
            "        (default: 1)\n",
            "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
            "        Size n-grams to block in beam search from the context. val <= 0\n",
            "        implies no blocking (default: -1)\n",
            "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
            "        Size n-grams to block in beam search. val <= 0 implies no blocking\n",
            "        (default: -1)\n",
            "  --beam-block-full-context BEAM_BLOCK_FULL_CONTEXT\n",
            "        Block n-grams from the *full* history context. Specify False to block\n",
            "        up to m tokens in the past, where m is truncation parameter for agent\n",
            "        (default: True)\n",
            "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
            "        Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
            "  --inference {greedy,topk,nucleus,delayedbeam,beam}\n",
            "        Generation algorithm (default: greedy)\n",
            "  --topk TOPK\n",
            "        K used in Top K sampling (default: 10)\n",
            "  --topp TOPP\n",
            "        p used in nucleus sampling (default: 0.9)\n",
            "  --beam-delay BEAM_DELAY\n",
            "        used in delayedbeam search (default: 30)\n",
            "  --beam-block-list-filename BEAM_BLOCK_LIST_FILENAME\n",
            "        Load a text file of hard blocks for beam search to never say.\n",
            "        (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "        temperature to add during decoding (default: 1.0)\n",
            "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
            "        if true, compute tokenized bleu scores (default: False)\n",
            "\n",
            "TorchAgent Arguments:\n",
            "  -i, --interactive-mode INTERACTIVE_MODE\n",
            "        Whether in full interactive mode or not, which means generating text\n",
            "        or retrieving from a full set of candidates, which is necessary to\n",
            "        actually do full dialogue. However, during training or quick\n",
            "        validation (e.g. PPL for generation or ranking a few candidates for\n",
            "        ranking models) you might want these set to off. Typically, scripts\n",
            "        can set their preferred default behavior at the start, e.g. eval\n",
            "        scripts. (default: False)\n",
            "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
            "        Choose between different strategies for initializing word embeddings.\n",
            "        Default is random, but can also preinitialize from Glove or Fasttext.\n",
            "        Preinitialized embeddings can also be fixed so they are not updated\n",
            "        during training. (default: random)\n",
            "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
            "        If pretrained embeddings have a different dimensionality than your\n",
            "        embedding size, strategy for projecting to the correct size. If the\n",
            "        dimensions are the same, this is ignored unless you append \"-force\" to\n",
            "        your choice. (default: random)\n",
            "  --fp16 FP16\n",
            "        Use fp16 computations. (default: False)\n",
            "  --fp16-impl {apex,mem_efficient}\n",
            "        Implementation of FP16 to use (default: apex)\n",
            "  -rc, --rank-candidates RANK_CANDIDATES\n",
            "        Whether the model should parse candidates for ranking. (default:\n",
            "        False)\n",
            "  -tr, --truncate TRUNCATE\n",
            "        Truncate input lengths to increase speed / use less memory. (default:\n",
            "        -1)\n",
            "  --text-truncate TEXT_TRUNCATE\n",
            "        Text input truncation length: if not specified, this will default to\n",
            "        `truncate` (default: None)\n",
            "  --label-truncate LABEL_TRUNCATE\n",
            "        Label truncation length: if not specified, this will default to\n",
            "        `truncate` (default: None)\n",
            "  --history-reversed HISTORY_REVERSED\n",
            "        Reverse the history (default: False)\n",
            "  -histsz, --history-size HISTORY_SIZE\n",
            "        Number of past dialog utterances to remember. (default: -1)\n",
            "  -pt, --person-tokens PERSON_TOKENS\n",
            "        add person tokens to history. adds __p1__ in front of input text and\n",
            "        __p2__ in front of past labels when available or past utterances\n",
            "        generated by the model. these are added to the dictionary during\n",
            "        initialization. (default: False)\n",
            "  --split-lines SPLIT_LINES\n",
            "        split the dialogue history on newlines and save in separate vectors\n",
            "        (default: False)\n",
            "  --delimiter DELIMITER\n",
            "        Join history lines with this token, defaults to newline (default: )\n",
            "  --special-tok-lst SPECIAL_TOK_LST\n",
            "        Comma separated list of special tokens (default: None)\n",
            "  -gpu, --gpu GPU\n",
            "        which GPU to use (default: -1)\n",
            "  --no-cuda\n",
            "        disable GPUs even if available. otherwise, will use GPUs if available\n",
            "        on the device.\n",
            "\n",
            "Optimizer Arguments:\n",
            "  -opt, --optimizer OPTIMIZER\n",
            "        Optimizer choice. Possible values: adadelta, adagrad, adam, adamw,\n",
            "        sparseadam, adamax, asgd, sgd, rprop, rmsprop, optimizer, lbfgs,\n",
            "        mem_eff_adam, adafactor. (default: sgd)\n",
            "  -lr, --learningrate LEARNINGRATE\n",
            "        Learning rate (default: 1)\n",
            "  -clip, --gradient-clip GRADIENT_CLIP\n",
            "        gradient clipping using l2 norm (default: 0.1)\n",
            "  --adafactor-eps ADAFACTOR_EPS\n",
            "        Epsilon values for adafactor optimizer: regularization constants for\n",
            "        square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
            "  -mom, --momentum MOMENTUM\n",
            "        if applicable, momentum value for optimizer. (default: 0)\n",
            "  --nesterov NESTEROV\n",
            "        if applicable, whether to use nesterov momentum. (default: True)\n",
            "  -nu, --nus NUS\n",
            "        if applicable, nu value(s) for optimizer. can use a single value like\n",
            "        0.7 or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
            "  -beta, --betas BETAS\n",
            "        if applicable, beta value(s) for optimizer. can use a single value\n",
            "        like 0.9 or a comma-separated tuple like 0.9,0.999 (default:\n",
            "        0.9,0.999)\n",
            "  -wdecay, --weight-decay WEIGHT_DECAY\n",
            "        Weight decay on the weights. (default: None)\n",
            "\n",
            "BPEHelper Arguments:\n",
            "  --bpe-vocab BPE_VOCAB\n",
            "        path to pre-trained tokenizer vocab (default: None)\n",
            "  --bpe-merge BPE_MERGE\n",
            "        path to pre-trained tokenizer merge (default: None)\n",
            "\n",
            "Learning Rate Scheduler:\n",
            "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
            "        Learning rate scheduler. (default: reduceonplateau)\n",
            "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
            "        LR scheduler patience. In number of validation runs. If using fixed\n",
            "        scheduler, LR is decayed every <patience> validations. (default: 3)\n",
            "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
            "        Decay factor for LR scheduler, or how much LR is multiplied by when it\n",
            "        is lowered. (default: 0.5)\n",
            "  --max-lr-steps MAX_LR_STEPS\n",
            "        Number of train steps the scheduler should take after warmup. Training\n",
            "        is terminated after this many steps. This should only be set for --lr-\n",
            "        scheduler cosine or linear (default: -1)\n",
            "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
            "        Constant used only to find the lr multiplier for the invsqrt\n",
            "        scheduler. Must be set for --lr-scheduler invsqrt (default: -1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGUWyKTwVtX"
      },
      "source": [
        "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLwGAq1wZJb"
      },
      "source": [
        "# Looking at model predictions\n",
        "\n",
        "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZgs6OlvJ-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71012c23-3ca9-4b0f-c5aa-3d124b8a61bd"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:24:15 | Using CUDA\n",
            "13:24:15 | loading dictionary from from_pretrained/model.dict\n",
            "13:24:15 | num words = 54944\n",
            "13:24:16 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:24:16 | Loading existing model params from from_pretrained/model\n",
            "13:24:25 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "13:24:26 | Opt:\n",
            "13:24:26 |     activation: gelu\n",
            "13:24:26 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "13:24:26 |     adam_eps: 1e-08\n",
            "13:24:26 |     add_p1_after_newln: False\n",
            "13:24:26 |     aggregate_micro: False\n",
            "13:24:26 |     allow_missing_init_opts: False\n",
            "13:24:26 |     attention_dropout: 0.0\n",
            "13:24:26 |     batchsize: 12\n",
            "13:24:26 |     beam_block_full_context: True\n",
            "13:24:26 |     beam_block_list_filename: None\n",
            "13:24:26 |     beam_block_ngram: -1\n",
            "13:24:26 |     beam_context_block_ngram: -1\n",
            "13:24:26 |     beam_delay: 30\n",
            "13:24:26 |     beam_length_penalty: 0.65\n",
            "13:24:26 |     beam_min_length: 1\n",
            "13:24:26 |     beam_size: 1\n",
            "13:24:26 |     betas: '[0.9, 0.999]'\n",
            "13:24:26 |     bpe_add_prefix_space: None\n",
            "13:24:26 |     bpe_debug: False\n",
            "13:24:26 |     bpe_merge: None\n",
            "13:24:26 |     bpe_vocab: None\n",
            "13:24:26 |     compute_tokenized_bleu: False\n",
            "13:24:26 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:24:26 |     datatype: train\n",
            "13:24:26 |     delimiter: '\\n'\n",
            "13:24:26 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:24:26 |     dict_endtoken: __end__\n",
            "13:24:26 |     dict_file: from_pretrained/model.dict\n",
            "13:24:26 |     dict_include_test: False\n",
            "13:24:26 |     dict_include_valid: False\n",
            "13:24:26 |     dict_initpath: None\n",
            "13:24:26 |     dict_language: english\n",
            "13:24:26 |     dict_loaded: True\n",
            "13:24:26 |     dict_lower: True\n",
            "13:24:26 |     dict_max_ngram_size: -1\n",
            "13:24:26 |     dict_maxexs: -1\n",
            "13:24:26 |     dict_maxtokens: -1\n",
            "13:24:26 |     dict_minfreq: 0\n",
            "13:24:26 |     dict_nulltoken: __null__\n",
            "13:24:26 |     dict_starttoken: __start__\n",
            "13:24:26 |     dict_textfields: text,labels\n",
            "13:24:26 |     dict_tokenizer: bpe\n",
            "13:24:26 |     dict_unktoken: __unk__\n",
            "13:24:26 |     display_examples: False\n",
            "13:24:26 |     display_ignore_fields: \n",
            "13:24:26 |     download_path: None\n",
            "13:24:26 |     dropout: 0.0\n",
            "13:24:26 |     dynamic_batching: full\n",
            "13:24:26 |     embedding_projection: random\n",
            "13:24:26 |     embedding_size: 512\n",
            "13:24:26 |     embedding_type: random\n",
            "13:24:26 |     embeddings_scale: True\n",
            "13:24:26 |     eval_batchsize: None\n",
            "13:24:26 |     evaltask: None\n",
            "13:24:26 |     ffn_size: 2048\n",
            "13:24:26 |     force_fp16_tokens: True\n",
            "13:24:26 |     fp16: True\n",
            "13:24:26 |     fp16_impl: mem_efficient\n",
            "13:24:26 |     gpu: -1\n",
            "13:24:26 |     gradient_clip: 0.1\n",
            "13:24:26 |     hf_skip_special_tokens: True\n",
            "13:24:26 |     hide_labels: False\n",
            "13:24:26 |     history_add_global_end_token: None\n",
            "13:24:26 |     history_reversed: False\n",
            "13:24:26 |     history_size: -1\n",
            "13:24:26 |     image_cropsize: 224\n",
            "13:24:26 |     image_mode: raw\n",
            "13:24:26 |     image_size: 256\n",
            "13:24:26 |     inference: greedy\n",
            "13:24:26 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "13:24:26 |     init_opt: None\n",
            "13:24:26 |     interactive_mode: False\n",
            "13:24:26 |     invsqrt_lr_decay_gamma: -1\n",
            "13:24:26 |     label_truncate: 128\n",
            "13:24:26 |     learn_positional_embeddings: True\n",
            "13:24:26 |     learningrate: 1e-05\n",
            "13:24:26 |     load_from_checkpoint: True\n",
            "13:24:26 |     log_every_n_secs: 10\n",
            "13:24:26 |     loglevel: info\n",
            "13:24:26 |     lr_scheduler: reduceonplateau\n",
            "13:24:26 |     lr_scheduler_decay: 0.5\n",
            "13:24:26 |     lr_scheduler_patience: 3\n",
            "13:24:26 |     max_lr_steps: -1\n",
            "13:24:26 |     max_train_time: 600.0\n",
            "13:24:26 |     metrics: default\n",
            "13:24:26 |     model: transformer/generator\n",
            "13:24:26 |     model_file: from_pretrained/model\n",
            "13:24:26 |     model_parallel: False\n",
            "13:24:26 |     momentum: 0\n",
            "13:24:26 |     multitask_weights: [1]\n",
            "13:24:26 |     n_decoder_layers: -1\n",
            "13:24:26 |     n_encoder_layers: -1\n",
            "13:24:26 |     n_heads: 16\n",
            "13:24:26 |     n_layers: 8\n",
            "13:24:26 |     n_positions: 512\n",
            "13:24:26 |     n_segments: 0\n",
            "13:24:26 |     nesterov: True\n",
            "13:24:26 |     no_cuda: False\n",
            "13:24:26 |     num_epochs: -1\n",
            "13:24:26 |     num_examples: 2\n",
            "13:24:26 |     nus: [0.7]\n",
            "13:24:26 |     optimizer: mem_eff_adam\n",
            "13:24:26 |     output_scaling: 1.0\n",
            "13:24:26 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model', 'num_examples': '2'}\"\n",
            "13:24:26 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:24:26 |     person_tokens: False\n",
            "13:24:26 |     rank_candidates: False\n",
            "13:24:26 |     relu_dropout: 0.0\n",
            "13:24:26 |     remove_political_convos: False\n",
            "13:24:26 |     save_after_valid: False\n",
            "13:24:26 |     save_every_n_secs: -1\n",
            "13:24:26 |     share_word_embeddings: True\n",
            "13:24:26 |     short_final_eval: False\n",
            "13:24:26 |     skip_generation: True\n",
            "13:24:26 |     special_tok_lst: None\n",
            "13:24:26 |     split_lines: False\n",
            "13:24:26 |     starttime: Feb07_13-12\n",
            "13:24:26 |     task: empathetic_dialogues\n",
            "13:24:26 |     temperature: 1.0\n",
            "13:24:26 |     tensorboard_log: False\n",
            "13:24:26 |     tensorboard_logdir: None\n",
            "13:24:26 |     text_truncate: 512\n",
            "13:24:26 |     topk: 10\n",
            "13:24:26 |     topp: 0.9\n",
            "13:24:26 |     train_experiencer_only: False\n",
            "13:24:26 |     truncate: -1\n",
            "13:24:26 |     update_freq: 1\n",
            "13:24:26 |     use_reply: label\n",
            "13:24:26 |     validation_cutoff: 1.0\n",
            "13:24:26 |     validation_every_n_epochs: 0.25\n",
            "13:24:26 |     validation_every_n_secs: -1\n",
            "13:24:26 |     validation_max_exs: -1\n",
            "13:24:26 |     validation_metric: ppl\n",
            "13:24:26 |     validation_metric_mode: None\n",
            "13:24:26 |     validation_patience: 10\n",
            "13:24:26 |     validation_share_agent: False\n",
            "13:24:26 |     variant: xlm\n",
            "13:24:26 |     verbose: False\n",
            "13:24:26 |     warmup_rate: 0.0001\n",
            "13:24:26 |     warmup_updates: 100\n",
            "13:24:26 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3QKTjdwokh"
      },
      "source": [
        "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLiq-vuowamh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5583af-88eb-4a36-9f61-da0258968d66"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:24:46 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "13:24:46 | Using CUDA\n",
            "13:24:46 | loading dictionary from from_pretrained/model.dict\n",
            "13:24:46 | num words = 54944\n",
            "13:24:48 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:24:48 | Loading existing model params from from_pretrained/model\n",
            "13:24:51 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "13:24:51 | Opt:\n",
            "13:24:51 |     activation: gelu\n",
            "13:24:51 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "13:24:51 |     adam_eps: 1e-08\n",
            "13:24:51 |     add_p1_after_newln: False\n",
            "13:24:51 |     aggregate_micro: False\n",
            "13:24:51 |     allow_missing_init_opts: False\n",
            "13:24:51 |     attention_dropout: 0.0\n",
            "13:24:51 |     batchsize: 12\n",
            "13:24:51 |     beam_block_full_context: True\n",
            "13:24:51 |     beam_block_list_filename: None\n",
            "13:24:51 |     beam_block_ngram: -1\n",
            "13:24:51 |     beam_context_block_ngram: -1\n",
            "13:24:51 |     beam_delay: 30\n",
            "13:24:51 |     beam_length_penalty: 0.65\n",
            "13:24:51 |     beam_min_length: 1\n",
            "13:24:51 |     beam_size: 1\n",
            "13:24:51 |     betas: '[0.9, 0.999]'\n",
            "13:24:51 |     bpe_add_prefix_space: None\n",
            "13:24:51 |     bpe_debug: False\n",
            "13:24:51 |     bpe_merge: None\n",
            "13:24:51 |     bpe_vocab: None\n",
            "13:24:51 |     compute_tokenized_bleu: False\n",
            "13:24:51 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:24:51 |     datatype: train\n",
            "13:24:51 |     delimiter: '\\n'\n",
            "13:24:51 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:24:51 |     dict_endtoken: __end__\n",
            "13:24:51 |     dict_file: from_pretrained/model.dict\n",
            "13:24:51 |     dict_include_test: False\n",
            "13:24:51 |     dict_include_valid: False\n",
            "13:24:51 |     dict_initpath: None\n",
            "13:24:51 |     dict_language: english\n",
            "13:24:51 |     dict_loaded: True\n",
            "13:24:51 |     dict_lower: True\n",
            "13:24:51 |     dict_max_ngram_size: -1\n",
            "13:24:51 |     dict_maxexs: -1\n",
            "13:24:51 |     dict_maxtokens: -1\n",
            "13:24:51 |     dict_minfreq: 0\n",
            "13:24:51 |     dict_nulltoken: __null__\n",
            "13:24:51 |     dict_starttoken: __start__\n",
            "13:24:51 |     dict_textfields: text,labels\n",
            "13:24:51 |     dict_tokenizer: bpe\n",
            "13:24:51 |     dict_unktoken: __unk__\n",
            "13:24:51 |     display_examples: False\n",
            "13:24:51 |     display_ignore_fields: \n",
            "13:24:51 |     download_path: None\n",
            "13:24:51 |     dropout: 0.0\n",
            "13:24:51 |     dynamic_batching: full\n",
            "13:24:51 |     embedding_projection: random\n",
            "13:24:51 |     embedding_size: 512\n",
            "13:24:51 |     embedding_type: random\n",
            "13:24:51 |     embeddings_scale: True\n",
            "13:24:51 |     eval_batchsize: None\n",
            "13:24:51 |     evaltask: None\n",
            "13:24:51 |     ffn_size: 2048\n",
            "13:24:51 |     force_fp16_tokens: True\n",
            "13:24:51 |     fp16: True\n",
            "13:24:51 |     fp16_impl: mem_efficient\n",
            "13:24:51 |     gpu: -1\n",
            "13:24:51 |     gradient_clip: 0.1\n",
            "13:24:51 |     hf_skip_special_tokens: True\n",
            "13:24:51 |     hide_labels: False\n",
            "13:24:51 |     history_add_global_end_token: None\n",
            "13:24:51 |     history_reversed: False\n",
            "13:24:51 |     history_size: -1\n",
            "13:24:51 |     image_cropsize: 224\n",
            "13:24:51 |     image_mode: raw\n",
            "13:24:51 |     image_size: 256\n",
            "13:24:51 |     inference: greedy\n",
            "13:24:51 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "13:24:51 |     init_opt: None\n",
            "13:24:51 |     interactive_mode: False\n",
            "13:24:51 |     invsqrt_lr_decay_gamma: -1\n",
            "13:24:51 |     label_truncate: 128\n",
            "13:24:51 |     learn_positional_embeddings: True\n",
            "13:24:51 |     learningrate: 1e-05\n",
            "13:24:51 |     load_from_checkpoint: True\n",
            "13:24:51 |     log_every_n_secs: 10\n",
            "13:24:51 |     loglevel: info\n",
            "13:24:51 |     lr_scheduler: reduceonplateau\n",
            "13:24:51 |     lr_scheduler_decay: 0.5\n",
            "13:24:51 |     lr_scheduler_patience: 3\n",
            "13:24:51 |     max_lr_steps: -1\n",
            "13:24:51 |     max_train_time: 600.0\n",
            "13:24:51 |     metrics: default\n",
            "13:24:51 |     model: transformer/generator\n",
            "13:24:51 |     model_file: from_pretrained/model\n",
            "13:24:51 |     model_parallel: False\n",
            "13:24:51 |     momentum: 0\n",
            "13:24:51 |     multitask_weights: [1]\n",
            "13:24:51 |     n_decoder_layers: -1\n",
            "13:24:51 |     n_encoder_layers: -1\n",
            "13:24:51 |     n_heads: 16\n",
            "13:24:51 |     n_layers: 8\n",
            "13:24:51 |     n_positions: 512\n",
            "13:24:51 |     n_segments: 0\n",
            "13:24:51 |     nesterov: True\n",
            "13:24:51 |     no_cuda: False\n",
            "13:24:51 |     num_epochs: -1\n",
            "13:24:51 |     num_examples: 2\n",
            "13:24:51 |     nus: [0.7]\n",
            "13:24:51 |     optimizer: mem_eff_adam\n",
            "13:24:51 |     output_scaling: 1.0\n",
            "13:24:51 |     override: \"{'task': 'empathetic_dialogues', 'model_file': 'from_pretrained/model', 'num_examples': '2', 'skip_generation': False}\"\n",
            "13:24:51 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:24:51 |     person_tokens: False\n",
            "13:24:51 |     rank_candidates: False\n",
            "13:24:51 |     relu_dropout: 0.0\n",
            "13:24:51 |     remove_political_convos: False\n",
            "13:24:51 |     save_after_valid: False\n",
            "13:24:51 |     save_every_n_secs: -1\n",
            "13:24:51 |     share_word_embeddings: True\n",
            "13:24:51 |     short_final_eval: False\n",
            "13:24:51 |     skip_generation: False\n",
            "13:24:51 |     special_tok_lst: None\n",
            "13:24:51 |     split_lines: False\n",
            "13:24:51 |     starttime: Feb07_13-12\n",
            "13:24:51 |     task: empathetic_dialogues\n",
            "13:24:51 |     temperature: 1.0\n",
            "13:24:51 |     tensorboard_log: False\n",
            "13:24:51 |     tensorboard_logdir: None\n",
            "13:24:51 |     text_truncate: 512\n",
            "13:24:51 |     topk: 10\n",
            "13:24:51 |     topp: 0.9\n",
            "13:24:51 |     train_experiencer_only: False\n",
            "13:24:51 |     truncate: -1\n",
            "13:24:51 |     update_freq: 1\n",
            "13:24:51 |     use_reply: label\n",
            "13:24:51 |     validation_cutoff: 1.0\n",
            "13:24:51 |     validation_every_n_epochs: 0.25\n",
            "13:24:51 |     validation_every_n_secs: -1\n",
            "13:24:51 |     validation_max_exs: -1\n",
            "13:24:51 |     validation_metric: ppl\n",
            "13:24:51 |     validation_metric_mode: None\n",
            "13:24:51 |     validation_patience: 10\n",
            "13:24:51 |     validation_share_agent: False\n",
            "13:24:51 |     variant: xlm\n",
            "13:24:51 |     verbose: False\n",
            "13:24:51 |     warmup_rate: 0.0001\n",
            "13:24:51 |     warmup_updates: 100\n",
            "13:24:51 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s scary ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good . i hope you are ok .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR0rn0ZwyxQ"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# Bringing your own datasets\n",
        "\n",
        "What if you want to build your own dataset in ParlAI? Of course you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0ea9d0-4934-4dc9-cc8a-c249d56b732a"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        # opt is the command line arguments.\n",
        "        \n",
        "        # What is this shared thing?\n",
        "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
        "        \n",
        "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
        "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
        "        # the fold name (train/valid/test) + \".txt\"\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # filename tells us where to load from.\n",
        "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data should yield tuples of ((text, label), new_episode)\n",
        "        # That is ((str, str), bool)\n",
        "        \n",
        "        # first episode\n",
        "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
        "        # in a conversation\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # second episode. We need to have True again!\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "             \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:25:07 | Opt:\n",
            "13:25:07 |     allow_missing_init_opts: False\n",
            "13:25:07 |     batchsize: 1\n",
            "13:25:07 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:25:07 |     datatype: train:ordered\n",
            "13:25:07 |     dict_class: None\n",
            "13:25:07 |     display_ignore_fields: agent_reply\n",
            "13:25:07 |     display_verbose: False\n",
            "13:25:07 |     download_path: None\n",
            "13:25:07 |     dynamic_batching: None\n",
            "13:25:07 |     hide_labels: False\n",
            "13:25:07 |     image_cropsize: 224\n",
            "13:25:07 |     image_mode: raw\n",
            "13:25:07 |     image_size: 256\n",
            "13:25:07 |     init_model: None\n",
            "13:25:07 |     init_opt: None\n",
            "13:25:07 |     loglevel: info\n",
            "13:25:07 |     max_display_len: 1000\n",
            "13:25:07 |     model: None\n",
            "13:25:07 |     model_file: None\n",
            "13:25:07 |     multitask_weights: [1]\n",
            "13:25:07 |     num_examples: 10\n",
            "13:25:07 |     override: \"{'task': 'my_teacher'}\"\n",
            "13:25:07 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:25:07 |     starttime: Feb07_13-25\n",
            "13:25:07 |     task: my_teacher\n",
            "13:25:07 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "13:25:07 | epoch done\n",
            "13:25:07 | loaded 2 episodes with a total of 6 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
        "\n",
        "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eae490a-00df-47d6-a63a-3f1a2658f9eb"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:25:14 | \u001b[33mOverriding opt[\"task\"] to my_teacher (previously: empathetic_dialogues)\u001b[0m\n",
            "13:25:14 | \u001b[33mOverriding opt[\"skip_generation\"] to False (previously: True)\u001b[0m\n",
            "13:25:14 | Using CUDA\n",
            "13:25:14 | loading dictionary from from_pretrained/model.dict\n",
            "13:25:14 | num words = 54944\n",
            "13:25:16 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "13:25:16 | Loading existing model params from from_pretrained/model\n",
            "13:25:19 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "13:25:19 | Opt:\n",
            "13:25:19 |     activation: gelu\n",
            "13:25:19 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "13:25:19 |     adam_eps: 1e-08\n",
            "13:25:19 |     add_p1_after_newln: False\n",
            "13:25:19 |     aggregate_micro: False\n",
            "13:25:19 |     allow_missing_init_opts: False\n",
            "13:25:19 |     attention_dropout: 0.0\n",
            "13:25:19 |     batchsize: 12\n",
            "13:25:19 |     beam_block_full_context: True\n",
            "13:25:19 |     beam_block_list_filename: None\n",
            "13:25:19 |     beam_block_ngram: -1\n",
            "13:25:19 |     beam_context_block_ngram: -1\n",
            "13:25:19 |     beam_delay: 30\n",
            "13:25:19 |     beam_length_penalty: 0.65\n",
            "13:25:19 |     beam_min_length: 1\n",
            "13:25:19 |     beam_size: 1\n",
            "13:25:19 |     betas: '[0.9, 0.999]'\n",
            "13:25:19 |     bpe_add_prefix_space: None\n",
            "13:25:19 |     bpe_debug: False\n",
            "13:25:19 |     bpe_merge: None\n",
            "13:25:19 |     bpe_vocab: None\n",
            "13:25:19 |     compute_tokenized_bleu: False\n",
            "13:25:19 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:25:19 |     datatype: train\n",
            "13:25:19 |     delimiter: '\\n'\n",
            "13:25:19 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:25:19 |     dict_endtoken: __end__\n",
            "13:25:19 |     dict_file: from_pretrained/model.dict\n",
            "13:25:19 |     dict_include_test: False\n",
            "13:25:19 |     dict_include_valid: False\n",
            "13:25:19 |     dict_initpath: None\n",
            "13:25:19 |     dict_language: english\n",
            "13:25:19 |     dict_loaded: True\n",
            "13:25:19 |     dict_lower: True\n",
            "13:25:19 |     dict_max_ngram_size: -1\n",
            "13:25:19 |     dict_maxexs: -1\n",
            "13:25:19 |     dict_maxtokens: -1\n",
            "13:25:19 |     dict_minfreq: 0\n",
            "13:25:19 |     dict_nulltoken: __null__\n",
            "13:25:19 |     dict_starttoken: __start__\n",
            "13:25:19 |     dict_textfields: text,labels\n",
            "13:25:19 |     dict_tokenizer: bpe\n",
            "13:25:19 |     dict_unktoken: __unk__\n",
            "13:25:19 |     display_examples: False\n",
            "13:25:19 |     display_ignore_fields: \n",
            "13:25:19 |     download_path: None\n",
            "13:25:19 |     dropout: 0.0\n",
            "13:25:19 |     dynamic_batching: full\n",
            "13:25:19 |     embedding_projection: random\n",
            "13:25:19 |     embedding_size: 512\n",
            "13:25:19 |     embedding_type: random\n",
            "13:25:19 |     embeddings_scale: True\n",
            "13:25:19 |     eval_batchsize: None\n",
            "13:25:19 |     evaltask: None\n",
            "13:25:19 |     ffn_size: 2048\n",
            "13:25:19 |     force_fp16_tokens: True\n",
            "13:25:19 |     fp16: True\n",
            "13:25:19 |     fp16_impl: mem_efficient\n",
            "13:25:19 |     gpu: -1\n",
            "13:25:19 |     gradient_clip: 0.1\n",
            "13:25:19 |     hf_skip_special_tokens: True\n",
            "13:25:19 |     hide_labels: False\n",
            "13:25:19 |     history_add_global_end_token: None\n",
            "13:25:19 |     history_reversed: False\n",
            "13:25:19 |     history_size: -1\n",
            "13:25:19 |     image_cropsize: 224\n",
            "13:25:19 |     image_mode: raw\n",
            "13:25:19 |     image_size: 256\n",
            "13:25:19 |     inference: greedy\n",
            "13:25:19 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "13:25:19 |     init_opt: None\n",
            "13:25:19 |     interactive_mode: False\n",
            "13:25:19 |     invsqrt_lr_decay_gamma: -1\n",
            "13:25:19 |     label_truncate: 128\n",
            "13:25:19 |     learn_positional_embeddings: True\n",
            "13:25:19 |     learningrate: 1e-05\n",
            "13:25:19 |     load_from_checkpoint: True\n",
            "13:25:19 |     log_every_n_secs: 10\n",
            "13:25:19 |     loglevel: info\n",
            "13:25:19 |     lr_scheduler: reduceonplateau\n",
            "13:25:19 |     lr_scheduler_decay: 0.5\n",
            "13:25:19 |     lr_scheduler_patience: 3\n",
            "13:25:19 |     max_lr_steps: -1\n",
            "13:25:19 |     max_train_time: 600.0\n",
            "13:25:19 |     metrics: default\n",
            "13:25:19 |     model: transformer/generator\n",
            "13:25:19 |     model_file: from_pretrained/model\n",
            "13:25:19 |     model_parallel: False\n",
            "13:25:19 |     momentum: 0\n",
            "13:25:19 |     multitask_weights: [1]\n",
            "13:25:19 |     n_decoder_layers: -1\n",
            "13:25:19 |     n_encoder_layers: -1\n",
            "13:25:19 |     n_heads: 16\n",
            "13:25:19 |     n_layers: 8\n",
            "13:25:19 |     n_positions: 512\n",
            "13:25:19 |     n_segments: 0\n",
            "13:25:19 |     nesterov: True\n",
            "13:25:19 |     no_cuda: False\n",
            "13:25:19 |     num_epochs: -1\n",
            "13:25:19 |     num_examples: 10\n",
            "13:25:19 |     nus: [0.7]\n",
            "13:25:19 |     optimizer: mem_eff_adam\n",
            "13:25:19 |     output_scaling: 1.0\n",
            "13:25:19 |     override: \"{'task': 'my_teacher', 'model_file': 'from_pretrained/model', 'skip_generation': False}\"\n",
            "13:25:19 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:25:19 |     person_tokens: False\n",
            "13:25:19 |     rank_candidates: False\n",
            "13:25:19 |     relu_dropout: 0.0\n",
            "13:25:19 |     remove_political_convos: False\n",
            "13:25:19 |     save_after_valid: False\n",
            "13:25:19 |     save_every_n_secs: -1\n",
            "13:25:19 |     share_word_embeddings: True\n",
            "13:25:19 |     short_final_eval: False\n",
            "13:25:19 |     skip_generation: False\n",
            "13:25:19 |     special_tok_lst: None\n",
            "13:25:19 |     split_lines: False\n",
            "13:25:19 |     starttime: Feb07_13-12\n",
            "13:25:19 |     task: my_teacher\n",
            "13:25:19 |     temperature: 1.0\n",
            "13:25:19 |     tensorboard_log: False\n",
            "13:25:19 |     tensorboard_logdir: None\n",
            "13:25:19 |     text_truncate: 512\n",
            "13:25:19 |     topk: 10\n",
            "13:25:19 |     topp: 0.9\n",
            "13:25:19 |     train_experiencer_only: False\n",
            "13:25:19 |     truncate: -1\n",
            "13:25:19 |     update_freq: 1\n",
            "13:25:19 |     use_reply: label\n",
            "13:25:19 |     validation_cutoff: 1.0\n",
            "13:25:19 |     validation_every_n_epochs: 0.25\n",
            "13:25:19 |     validation_every_n_secs: -1\n",
            "13:25:19 |     validation_max_exs: -1\n",
            "13:25:19 |     validation_metric: ppl\n",
            "13:25:19 |     validation_metric_mode: None\n",
            "13:25:19 |     validation_patience: 10\n",
            "13:25:19 |     validation_share_agent: False\n",
            "13:25:19 |     variant: xlm\n",
            "13:25:19 |     verbose: False\n",
            "13:25:19 |     warmup_rate: 0.0001\n",
            "13:25:19 |     warmup_updates: 100\n",
            "13:25:19 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' m good , how about you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "13:25:20 | epoch done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzvSHAy0meK"
      },
      "source": [
        "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# Creating your own models\n",
        "\n",
        "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # Gather the last word from the other user's input\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # Always return a string like this.\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "Let's try seeing how this agent behaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e93aaab7-9c4c-4fc6-e82e-cd7fa5e9181e"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:25:39 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "13:25:39 | Opt:\n",
            "13:25:39 |     allow_missing_init_opts: False\n",
            "13:25:39 |     batchsize: 1\n",
            "13:25:39 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:25:39 |     datatype: valid\n",
            "13:25:39 |     dict_class: None\n",
            "13:25:39 |     display_ignore_fields: \n",
            "13:25:39 |     download_path: None\n",
            "13:25:39 |     dynamic_batching: None\n",
            "13:25:39 |     hide_labels: False\n",
            "13:25:39 |     image_cropsize: 224\n",
            "13:25:39 |     image_mode: raw\n",
            "13:25:39 |     image_size: 256\n",
            "13:25:39 |     init_model: None\n",
            "13:25:39 |     init_opt: None\n",
            "13:25:39 |     loglevel: info\n",
            "13:25:39 |     model: hello\n",
            "13:25:39 |     model_file: None\n",
            "13:25:39 |     multitask_weights: [1]\n",
            "13:25:39 |     name: Alice\n",
            "13:25:39 |     num_examples: 10\n",
            "13:25:39 |     override: \"{'task': 'my_teacher', 'model': 'hello'}\"\n",
            "13:25:39 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:25:39 |     starttime: Feb07_13-25\n",
            "13:25:39 |     task: my_teacher\n",
            "13:25:39 |     verbose: False\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
            "13:25:39 | epoch done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf132a5-0f97-41fa-e483-edd9bde2aa9e"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:25:46 | Opt:\n",
            "13:25:46 |     allow_missing_init_opts: False\n",
            "13:25:46 |     batchsize: 1\n",
            "13:25:46 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:25:46 |     datatype: train\n",
            "13:25:46 |     dict_class: None\n",
            "13:25:46 |     display_examples: False\n",
            "13:25:46 |     display_ignore_fields: label_candidates,text_candidates\n",
            "13:25:46 |     display_prettify: False\n",
            "13:25:46 |     download_path: None\n",
            "13:25:46 |     dynamic_batching: None\n",
            "13:25:46 |     hide_labels: False\n",
            "13:25:46 |     image_cropsize: 224\n",
            "13:25:46 |     image_mode: raw\n",
            "13:25:46 |     image_size: 256\n",
            "13:25:46 |     init_model: None\n",
            "13:25:46 |     init_opt: None\n",
            "13:25:46 |     interactive_mode: True\n",
            "13:25:46 |     interactive_task: True\n",
            "13:25:46 |     local_human_candidates_file: None\n",
            "13:25:46 |     log_keep_fields: all\n",
            "13:25:46 |     loglevel: info\n",
            "13:25:46 |     model: hello\n",
            "13:25:46 |     model_file: None\n",
            "13:25:46 |     multitask_weights: [1]\n",
            "13:25:46 |     name: Bob\n",
            "13:25:46 |     outfile: \n",
            "13:25:46 |     override: \"{'model': 'hello', 'name': 'Bob'}\"\n",
            "13:25:46 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:25:46 |     save_format: conversations\n",
            "13:25:46 |     single_turn: False\n",
            "13:25:46 |     starttime: Feb07_13-25\n",
            "13:25:46 |     task: interactive\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "13:25:46 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hello\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello hello, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m what can you do?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello do?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m oh...\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello oh..., I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAe1hytf1BPk"
      },
      "source": [
        "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## Creating a neural network model\n",
        "\n",
        "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
        "\n",
        "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # must call super on all nn.Modules.\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # this is our very first call. We want to seed the LSTM with the\n",
        "            # hidden state of the decoder\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # We've generated some tokens already, so we can reuse the existing\n",
        "            # decoder state\n",
        "            state = incr_state\n",
        "\n",
        "        # get the new output and decoder incremental state\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
        "        super(Seq2seqAgent, cls).add_cmdline_args(argparser)\n",
        "\n",
        "        # Add custom arguments only for this model.\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # Optionally initialize pre-trained embeddings by copying them from another\n",
        "        # source: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15aa7204-f6d6-48f9-8254-d2481fb78eca"
      },
      "source": [
        "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:27:19 | building dictionary first...\n",
            "13:27:19 | Opt:\n",
            "13:27:19 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:27:19 |     adam_eps: 1e-08\n",
            "13:27:19 |     add_p1_after_newln: False\n",
            "13:27:19 |     aggregate_micro: False\n",
            "13:27:19 |     allow_missing_init_opts: False\n",
            "13:27:19 |     batchsize: 1\n",
            "13:27:19 |     beam_block_full_context: True\n",
            "13:27:19 |     beam_block_list_filename: None\n",
            "13:27:19 |     beam_block_ngram: -1\n",
            "13:27:19 |     beam_context_block_ngram: -1\n",
            "13:27:19 |     beam_delay: 30\n",
            "13:27:19 |     beam_length_penalty: 0.65\n",
            "13:27:19 |     beam_min_length: 1\n",
            "13:27:19 |     beam_size: 1\n",
            "13:27:19 |     betas: '(0.9, 0.999)'\n",
            "13:27:19 |     bpe_add_prefix_space: None\n",
            "13:27:19 |     bpe_debug: False\n",
            "13:27:19 |     bpe_merge: None\n",
            "13:27:19 |     bpe_vocab: None\n",
            "13:27:19 |     compute_tokenized_bleu: False\n",
            "13:27:19 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:27:19 |     datatype: train\n",
            "13:27:19 |     delimiter: '\\n'\n",
            "13:27:19 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:27:19 |     dict_endtoken: __end__\n",
            "13:27:19 |     dict_file: my_first_lstm/model.dict\n",
            "13:27:19 |     dict_include_test: False\n",
            "13:27:19 |     dict_include_valid: False\n",
            "13:27:19 |     dict_initpath: None\n",
            "13:27:19 |     dict_language: english\n",
            "13:27:19 |     dict_loaded: False\n",
            "13:27:19 |     dict_lower: False\n",
            "13:27:19 |     dict_max_ngram_size: -1\n",
            "13:27:19 |     dict_maxexs: -1\n",
            "13:27:19 |     dict_maxtokens: -1\n",
            "13:27:19 |     dict_minfreq: 0\n",
            "13:27:19 |     dict_nulltoken: __null__\n",
            "13:27:19 |     dict_starttoken: __start__\n",
            "13:27:19 |     dict_textfields: text,labels\n",
            "13:27:19 |     dict_tokenizer: re\n",
            "13:27:19 |     dict_unktoken: __unk__\n",
            "13:27:19 |     display_examples: False\n",
            "13:27:19 |     download_path: None\n",
            "13:27:19 |     dynamic_batching: None\n",
            "13:27:19 |     embedding_projection: random\n",
            "13:27:19 |     embedding_type: random\n",
            "13:27:19 |     eval_batchsize: None\n",
            "13:27:19 |     evaltask: None\n",
            "13:27:19 |     force_fp16_tokens: False\n",
            "13:27:19 |     fp16: False\n",
            "13:27:19 |     fp16_impl: apex\n",
            "13:27:19 |     gpu: -1\n",
            "13:27:19 |     gradient_clip: 0.1\n",
            "13:27:19 |     hf_skip_special_tokens: True\n",
            "13:27:19 |     hidden_size: 1024\n",
            "13:27:19 |     hide_labels: False\n",
            "13:27:19 |     history_add_global_end_token: None\n",
            "13:27:19 |     history_reversed: False\n",
            "13:27:19 |     history_size: -1\n",
            "13:27:19 |     image_cropsize: 224\n",
            "13:27:19 |     image_mode: no_image_model\n",
            "13:27:19 |     image_size: 256\n",
            "13:27:19 |     inference: greedy\n",
            "13:27:19 |     init_model: None\n",
            "13:27:19 |     init_opt: None\n",
            "13:27:19 |     interactive_mode: False\n",
            "13:27:19 |     invsqrt_lr_decay_gamma: -1\n",
            "13:27:19 |     label_truncate: None\n",
            "13:27:19 |     learningrate: 1\n",
            "13:27:19 |     load_from_checkpoint: True\n",
            "13:27:19 |     log_every_n_secs: 10\n",
            "13:27:19 |     loglevel: info\n",
            "13:27:19 |     lr_scheduler: reduceonplateau\n",
            "13:27:19 |     lr_scheduler_decay: 0.5\n",
            "13:27:19 |     lr_scheduler_patience: 3\n",
            "13:27:19 |     max_lr_steps: -1\n",
            "13:27:19 |     max_train_time: 60.0\n",
            "13:27:19 |     metrics: default\n",
            "13:27:19 |     model: my_first_lstm\n",
            "13:27:19 |     model_file: my_first_lstm/model\n",
            "13:27:19 |     momentum: 0\n",
            "13:27:19 |     multitask_weights: [1]\n",
            "13:27:19 |     nesterov: True\n",
            "13:27:19 |     no_cuda: False\n",
            "13:27:19 |     num_epochs: -1\n",
            "13:27:19 |     nus: (0.7,)\n",
            "13:27:19 |     optimizer: sgd\n",
            "13:27:19 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'my_teacher', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
            "13:27:19 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:27:19 |     person_tokens: False\n",
            "13:27:19 |     rank_candidates: False\n",
            "13:27:19 |     save_after_valid: False\n",
            "13:27:19 |     save_every_n_secs: -1\n",
            "13:27:19 |     short_final_eval: False\n",
            "13:27:19 |     skip_generation: False\n",
            "13:27:19 |     special_tok_lst: None\n",
            "13:27:19 |     split_lines: False\n",
            "13:27:19 |     starttime: Feb07_13-27\n",
            "13:27:19 |     task: my_teacher\n",
            "13:27:19 |     temperature: 1.0\n",
            "13:27:19 |     tensorboard_log: False\n",
            "13:27:19 |     tensorboard_logdir: None\n",
            "13:27:19 |     text_truncate: None\n",
            "13:27:19 |     topk: 10\n",
            "13:27:19 |     topp: 0.9\n",
            "13:27:19 |     truncate: -1\n",
            "13:27:19 |     update_freq: 1\n",
            "13:27:19 |     use_reply: label\n",
            "13:27:19 |     validation_cutoff: 1.0\n",
            "13:27:19 |     validation_every_n_epochs: -1\n",
            "13:27:19 |     validation_every_n_secs: 10.0\n",
            "13:27:19 |     validation_max_exs: -1\n",
            "13:27:19 |     validation_metric: accuracy\n",
            "13:27:19 |     validation_metric_mode: None\n",
            "13:27:19 |     validation_patience: 10\n",
            "13:27:19 |     validation_share_agent: False\n",
            "13:27:19 |     warmup_rate: 0.0001\n",
            "13:27:19 |     warmup_updates: -1\n",
            "13:27:19 |     weight_decay: None\n",
            "13:27:20 | creating task(s): my_teacher\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 2.39kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ~~ Loading from train.txt ~~ \n",
            " ~~ Loading from train.txt ~~ \n",
            "13:27:20 | Saving dictionary to my_first_lstm/model.dict\n",
            "13:27:20 | dictionary built with 30 tokens in 0.0s\n",
            "13:27:20 | No model with opt yet at: my_first_lstm/model(.opt)\n",
            "13:27:20 | Using CUDA\n",
            "13:27:20 | loading dictionary from my_first_lstm/model.dict\n",
            "13:27:20 | num words = 30\n",
            "13:27:20 | Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "13:27:20 | Opt:\n",
            "13:27:20 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "13:27:20 |     adam_eps: 1e-08\n",
            "13:27:20 |     add_p1_after_newln: False\n",
            "13:27:20 |     aggregate_micro: False\n",
            "13:27:20 |     allow_missing_init_opts: False\n",
            "13:27:20 |     batchsize: 1\n",
            "13:27:20 |     beam_block_full_context: True\n",
            "13:27:20 |     beam_block_list_filename: None\n",
            "13:27:20 |     beam_block_ngram: -1\n",
            "13:27:20 |     beam_context_block_ngram: -1\n",
            "13:27:20 |     beam_delay: 30\n",
            "13:27:20 |     beam_length_penalty: 0.65\n",
            "13:27:20 |     beam_min_length: 1\n",
            "13:27:20 |     beam_size: 1\n",
            "13:27:20 |     betas: '(0.9, 0.999)'\n",
            "13:27:20 |     bpe_add_prefix_space: None\n",
            "13:27:20 |     bpe_debug: False\n",
            "13:27:20 |     bpe_merge: None\n",
            "13:27:20 |     bpe_vocab: None\n",
            "13:27:20 |     compute_tokenized_bleu: False\n",
            "13:27:20 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:27:20 |     datatype: train\n",
            "13:27:20 |     delimiter: '\\n'\n",
            "13:27:20 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:27:20 |     dict_endtoken: __end__\n",
            "13:27:20 |     dict_file: my_first_lstm/model.dict\n",
            "13:27:20 |     dict_include_test: False\n",
            "13:27:20 |     dict_include_valid: False\n",
            "13:27:20 |     dict_initpath: None\n",
            "13:27:20 |     dict_language: english\n",
            "13:27:20 |     dict_loaded: True\n",
            "13:27:20 |     dict_lower: False\n",
            "13:27:20 |     dict_max_ngram_size: -1\n",
            "13:27:20 |     dict_maxexs: -1\n",
            "13:27:20 |     dict_maxtokens: -1\n",
            "13:27:20 |     dict_minfreq: 0\n",
            "13:27:20 |     dict_nulltoken: __null__\n",
            "13:27:20 |     dict_starttoken: __start__\n",
            "13:27:20 |     dict_textfields: text,labels\n",
            "13:27:20 |     dict_tokenizer: re\n",
            "13:27:20 |     dict_unktoken: __unk__\n",
            "13:27:20 |     display_examples: False\n",
            "13:27:20 |     download_path: None\n",
            "13:27:20 |     dynamic_batching: None\n",
            "13:27:20 |     embedding_projection: random\n",
            "13:27:20 |     embedding_type: random\n",
            "13:27:20 |     eval_batchsize: None\n",
            "13:27:20 |     evaltask: None\n",
            "13:27:20 |     force_fp16_tokens: False\n",
            "13:27:20 |     fp16: False\n",
            "13:27:20 |     fp16_impl: apex\n",
            "13:27:20 |     gpu: -1\n",
            "13:27:20 |     gradient_clip: 0.1\n",
            "13:27:20 |     hf_skip_special_tokens: True\n",
            "13:27:20 |     hidden_size: 1024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13:27:20 |     hide_labels: False\n",
            "13:27:20 |     history_add_global_end_token: None\n",
            "13:27:20 |     history_reversed: False\n",
            "13:27:20 |     history_size: -1\n",
            "13:27:20 |     image_cropsize: 224\n",
            "13:27:20 |     image_mode: raw\n",
            "13:27:20 |     image_size: 256\n",
            "13:27:20 |     inference: greedy\n",
            "13:27:20 |     init_model: None\n",
            "13:27:20 |     init_opt: None\n",
            "13:27:20 |     interactive_mode: False\n",
            "13:27:20 |     invsqrt_lr_decay_gamma: -1\n",
            "13:27:20 |     label_truncate: None\n",
            "13:27:20 |     learningrate: 1\n",
            "13:27:20 |     load_from_checkpoint: True\n",
            "13:27:20 |     log_every_n_secs: 10\n",
            "13:27:20 |     loglevel: info\n",
            "13:27:20 |     lr_scheduler: reduceonplateau\n",
            "13:27:20 |     lr_scheduler_decay: 0.5\n",
            "13:27:20 |     lr_scheduler_patience: 3\n",
            "13:27:20 |     max_lr_steps: -1\n",
            "13:27:20 |     max_train_time: 60.0\n",
            "13:27:20 |     metrics: default\n",
            "13:27:20 |     model: my_first_lstm\n",
            "13:27:20 |     model_file: my_first_lstm/model\n",
            "13:27:20 |     momentum: 0\n",
            "13:27:20 |     multitask_weights: [1]\n",
            "13:27:20 |     nesterov: True\n",
            "13:27:20 |     no_cuda: False\n",
            "13:27:20 |     num_epochs: -1\n",
            "13:27:20 |     nus: (0.7,)\n",
            "13:27:20 |     optimizer: sgd\n",
            "13:27:20 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'my_teacher', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
            "13:27:20 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:27:20 |     person_tokens: False\n",
            "13:27:20 |     rank_candidates: False\n",
            "13:27:20 |     save_after_valid: False\n",
            "13:27:20 |     save_every_n_secs: -1\n",
            "13:27:20 |     short_final_eval: False\n",
            "13:27:20 |     skip_generation: False\n",
            "13:27:20 |     special_tok_lst: None\n",
            "13:27:20 |     split_lines: False\n",
            "13:27:20 |     starttime: Feb07_13-27\n",
            "13:27:20 |     task: my_teacher\n",
            "13:27:20 |     temperature: 1.0\n",
            "13:27:20 |     tensorboard_log: False\n",
            "13:27:20 |     tensorboard_logdir: None\n",
            "13:27:20 |     text_truncate: None\n",
            "13:27:20 |     topk: 10\n",
            "13:27:20 |     topp: 0.9\n",
            "13:27:20 |     truncate: -1\n",
            "13:27:20 |     update_freq: 1\n",
            "13:27:20 |     use_reply: label\n",
            "13:27:20 |     validation_cutoff: 1.0\n",
            "13:27:20 |     validation_every_n_epochs: -1\n",
            "13:27:20 |     validation_every_n_secs: 10.0\n",
            "13:27:20 |     validation_max_exs: -1\n",
            "13:27:20 |     validation_metric: accuracy\n",
            "13:27:20 |     validation_metric_mode: None\n",
            "13:27:20 |     validation_patience: 10\n",
            "13:27:20 |     validation_share_agent: False\n",
            "13:27:20 |     warmup_rate: 0.0001\n",
            "13:27:20 |     warmup_updates: -1\n",
            "13:27:20 |     weight_decay: None\n",
            "13:27:20 | creating task(s): my_teacher\n",
            " ~~ Loading from train.txt ~~ \n",
            "13:27:20 | training...\n",
            "13:27:30 | time:10s total_exs:1258 epochs:209.67\n",
            "     clip  ctpb  ctps  exps  exs  gnorm  gpu_mem   loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates   tpb  tps   ups\n",
            "   .02623 8.173  1028 125.8 1258  2.415   .07801 .07225   1 3.308 416.3 1.075      .9909                 1258 11.48 1445 125.8\n",
            "\n",
            "13:27:30 | time:10s total_exs:1258 epochs:209.67\n",
            "    gpu_mem  lr  total_train_updates\n",
            "    .008579   1                 1258\n",
            "\n",
            "13:27:30 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "13:27:30 | running eval: valid\n",
            "13:27:30 | eval completed in 0.07s\n",
            "13:27:30 | \u001b[1mvalid:\n",
            "    accuracy   bleu-4  ctpb  ctps  exps  exs  f1  gpu_mem      loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb  \\\n",
            "           1 .0003337 8.167 827.6 100.3    6   1  .008575 7.749e-08   1 3.333 335.5    1          1                 1258 11.5   \n",
            "    tps  \n",
            "   1166\n",
            "\u001b[0m\n",
            "13:27:30 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n",
            "13:27:30 | saving best valid model: my_first_lstm/model\n",
            "13:27:31 | task solved! stopping.\n",
            "13:27:31 | Using CUDA\n",
            "13:27:31 | loading dictionary from my_first_lstm/model.dict\n",
            "13:27:31 | num words = 30\n",
            "13:27:31 | Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "13:27:31 | Loading existing model params from my_first_lstm/model\n",
            "13:27:31 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "13:27:31 | running eval: valid\n",
            "13:27:31 | eval completed in 0.07s\n",
            "13:27:31 | \u001b[1mvalid:\n",
            "    accuracy   bleu-4  ctpb  ctps  exps  exs  f1  gpu_mem      loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb  \\\n",
            "           1 .0003337 8.167 838.2 100.9    6   1   .00432 7.749e-08   1 3.333   338    1          1                 1258 11.5   \n",
            "    tps  \n",
            "   1181\n",
            "\u001b[0m\n",
            "13:27:31 | creating task(s): my_teacher\n",
            " ~~ Loading from test.txt ~~ \n",
            "13:27:31 | running eval: test\n",
            "13:27:31 | eval completed in 0.07s\n",
            "13:27:31 | \u001b[1mtest:\n",
            "    accuracy   bleu-4  ctpb  ctps  exps  exs  f1  gpu_mem      loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates  tpb  \\\n",
            "           1 .0003337 8.167 831.9 101.1    6   1  .004319 7.749e-08   1 3.333 338.3    1          1                 1258 11.5   \n",
            "    tps  \n",
            "   1172\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(1),\n",
              "  'bleu-4': BleuMetric(0.0003337),\n",
              "  'ctpb': GlobalAverageMetric(8.167),\n",
              "  'ctps': GlobalTimerMetric(838.2),\n",
              "  'exps': GlobalTimerMetric(100.9),\n",
              "  'exs': SumMetric(6),\n",
              "  'f1': F1Metric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.00432),\n",
              "  'loss': AverageMetric(7.749e-08),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(3.333),\n",
              "  'ltps': GlobalTimerMetric(338),\n",
              "  'ppl': PPLMetric(1),\n",
              "  'token_acc': AverageMetric(1),\n",
              "  'total_train_updates': GlobalFixedMetric(1258),\n",
              "  'tpb': GlobalAverageMetric(11.5),\n",
              "  'tps': GlobalTimerMetric(1181)},\n",
              " {'accuracy': ExactMatchMetric(1),\n",
              "  'bleu-4': BleuMetric(0.0003337),\n",
              "  'ctpb': GlobalAverageMetric(8.167),\n",
              "  'ctps': GlobalTimerMetric(831.9),\n",
              "  'exps': GlobalTimerMetric(101.1),\n",
              "  'exs': SumMetric(6),\n",
              "  'f1': F1Metric(1),\n",
              "  'gpu_mem': GlobalAverageMetric(0.004319),\n",
              "  'loss': AverageMetric(7.749e-08),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(3.333),\n",
              "  'ltps': GlobalTimerMetric(338.3),\n",
              "  'ppl': PPLMetric(1),\n",
              "  'token_acc': AverageMetric(1),\n",
              "  'total_train_updates': GlobalFixedMetric(1258),\n",
              "  'tpb': GlobalAverageMetric(11.5),\n",
              "  'tps': GlobalTimerMetric(1172)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "Let's see how it does. It should reproduce the data perfectly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24c49c3-9b0b-4cf2-9e91-c179d25673ea"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13:27:43 | Using CUDA\n",
            "13:27:43 | loading dictionary from my_first_lstm/model.dict\n",
            "13:27:43 | num words = 30\n",
            "13:27:43 | Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "13:27:43 | Loading existing model params from my_first_lstm/model\n",
            "13:27:43 | creating task(s): my_teacher\n",
            " ~~ Loading from valid.txt ~~ \n",
            "13:27:43 | Opt:\n",
            "13:27:43 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "13:27:43 |     adam_eps: 1e-08\n",
            "13:27:43 |     add_p1_after_newln: False\n",
            "13:27:43 |     aggregate_micro: False\n",
            "13:27:43 |     allow_missing_init_opts: False\n",
            "13:27:43 |     batchsize: 1\n",
            "13:27:43 |     beam_block_full_context: True\n",
            "13:27:43 |     beam_block_list_filename: None\n",
            "13:27:43 |     beam_block_ngram: -1\n",
            "13:27:43 |     beam_context_block_ngram: -1\n",
            "13:27:43 |     beam_delay: 30\n",
            "13:27:43 |     beam_length_penalty: 0.65\n",
            "13:27:43 |     beam_min_length: 1\n",
            "13:27:43 |     beam_size: 1\n",
            "13:27:43 |     betas: '[0.9, 0.999]'\n",
            "13:27:43 |     bpe_add_prefix_space: None\n",
            "13:27:43 |     bpe_debug: False\n",
            "13:27:43 |     bpe_merge: None\n",
            "13:27:43 |     bpe_vocab: None\n",
            "13:27:43 |     compute_tokenized_bleu: False\n",
            "13:27:43 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "13:27:43 |     datatype: train\n",
            "13:27:43 |     delimiter: '\\n'\n",
            "13:27:43 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "13:27:43 |     dict_endtoken: __end__\n",
            "13:27:43 |     dict_file: my_first_lstm/model.dict\n",
            "13:27:43 |     dict_include_test: False\n",
            "13:27:43 |     dict_include_valid: False\n",
            "13:27:43 |     dict_initpath: None\n",
            "13:27:43 |     dict_language: english\n",
            "13:27:43 |     dict_loaded: True\n",
            "13:27:43 |     dict_lower: False\n",
            "13:27:43 |     dict_max_ngram_size: -1\n",
            "13:27:43 |     dict_maxexs: -1\n",
            "13:27:43 |     dict_maxtokens: -1\n",
            "13:27:43 |     dict_minfreq: 0\n",
            "13:27:43 |     dict_nulltoken: __null__\n",
            "13:27:43 |     dict_starttoken: __start__\n",
            "13:27:43 |     dict_textfields: text,labels\n",
            "13:27:43 |     dict_tokenizer: re\n",
            "13:27:43 |     dict_unktoken: __unk__\n",
            "13:27:43 |     display_examples: False\n",
            "13:27:43 |     display_ignore_fields: \n",
            "13:27:43 |     download_path: None\n",
            "13:27:43 |     dynamic_batching: None\n",
            "13:27:43 |     embedding_projection: random\n",
            "13:27:43 |     embedding_type: random\n",
            "13:27:43 |     eval_batchsize: None\n",
            "13:27:43 |     evaltask: None\n",
            "13:27:43 |     force_fp16_tokens: False\n",
            "13:27:43 |     fp16: False\n",
            "13:27:43 |     fp16_impl: apex\n",
            "13:27:43 |     gpu: -1\n",
            "13:27:43 |     gradient_clip: 0.1\n",
            "13:27:43 |     hf_skip_special_tokens: True\n",
            "13:27:43 |     hidden_size: 1024\n",
            "13:27:43 |     hide_labels: False\n",
            "13:27:43 |     history_add_global_end_token: None\n",
            "13:27:43 |     history_reversed: False\n",
            "13:27:43 |     history_size: -1\n",
            "13:27:43 |     image_cropsize: 224\n",
            "13:27:43 |     image_mode: raw\n",
            "13:27:43 |     image_size: 256\n",
            "13:27:43 |     inference: greedy\n",
            "13:27:43 |     init_model: None\n",
            "13:27:43 |     init_opt: None\n",
            "13:27:43 |     interactive_mode: False\n",
            "13:27:43 |     invsqrt_lr_decay_gamma: -1\n",
            "13:27:43 |     label_truncate: None\n",
            "13:27:43 |     learningrate: 1\n",
            "13:27:43 |     load_from_checkpoint: True\n",
            "13:27:43 |     log_every_n_secs: 10\n",
            "13:27:43 |     loglevel: info\n",
            "13:27:43 |     lr_scheduler: reduceonplateau\n",
            "13:27:43 |     lr_scheduler_decay: 0.5\n",
            "13:27:43 |     lr_scheduler_patience: 3\n",
            "13:27:43 |     max_lr_steps: -1\n",
            "13:27:43 |     max_train_time: 60.0\n",
            "13:27:43 |     metrics: default\n",
            "13:27:43 |     model: my_first_lstm\n",
            "13:27:43 |     model_file: my_first_lstm/model\n",
            "13:27:43 |     momentum: 0\n",
            "13:27:43 |     multitask_weights: [1]\n",
            "13:27:43 |     nesterov: True\n",
            "13:27:43 |     no_cuda: False\n",
            "13:27:43 |     num_epochs: -1\n",
            "13:27:43 |     num_examples: 10\n",
            "13:27:43 |     nus: [0.7]\n",
            "13:27:43 |     optimizer: sgd\n",
            "13:27:43 |     override: \"{'model_file': 'my_first_lstm/model', 'task': 'my_teacher'}\"\n",
            "13:27:43 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "13:27:43 |     person_tokens: False\n",
            "13:27:43 |     rank_candidates: False\n",
            "13:27:43 |     save_after_valid: False\n",
            "13:27:43 |     save_every_n_secs: -1\n",
            "13:27:43 |     short_final_eval: False\n",
            "13:27:43 |     skip_generation: False\n",
            "13:27:43 |     special_tok_lst: None\n",
            "13:27:43 |     split_lines: False\n",
            "13:27:43 |     starttime: Feb07_13-27\n",
            "13:27:43 |     task: my_teacher\n",
            "13:27:43 |     temperature: 1.0\n",
            "13:27:43 |     tensorboard_log: False\n",
            "13:27:43 |     tensorboard_logdir: None\n",
            "13:27:43 |     text_truncate: None\n",
            "13:27:43 |     topk: 10\n",
            "13:27:43 |     topp: 0.9\n",
            "13:27:43 |     truncate: -1\n",
            "13:27:43 |     update_freq: 1\n",
            "13:27:43 |     use_reply: label\n",
            "13:27:43 |     validation_cutoff: 1.0\n",
            "13:27:43 |     validation_every_n_epochs: -1\n",
            "13:27:43 |     validation_every_n_secs: 10.0\n",
            "13:27:43 |     validation_max_exs: -1\n",
            "13:27:43 |     validation_metric: accuracy\n",
            "13:27:43 |     validation_metric_mode: None\n",
            "13:27:43 |     validation_patience: 10\n",
            "13:27:43 |     validation_share_agent: False\n",
            "13:27:43 |     verbose: False\n",
            "13:27:43 |     warmup_rate: 0.0001\n",
            "13:27:43 |     warmup_updates: -1\n",
            "13:27:43 |     weight_decay: None\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "13:27:44 | epoch done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzrDhPS1QCW"
      },
      "source": [
        "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
        "\n",
        "# What's next!\n",
        "\n",
        "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
        "\n",
        "Here are some other great resources:\n",
        "- [Our research page](https://parl.ai/projects/)\n",
        "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
        "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
        "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
        "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    }
  ]
}